{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    tokens = doc.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yertle', 'the', 'turtle', 'on', 'the', 'far', 'away', 'island', 'of', 'sala', 'ma', 'sond', 'yertle', 'the', 'turtle', 'was', 'king', 'of', 'the', 'pond.', 'a', 'nice', 'little', 'pond.', 'it', 'was', 'clean.', 'it', 'was', 'neat.', 'the', 'water', 'was', 'warm.', 'there', 'was', 'plenty', 'to', 'eat.', 'the', 'turtles', 'had', 'everything', 'turtles', 'might', 'need.', 'and', 'they', 'were', 'all', 'happy.', 'quite', 'happy', 'indeed.', 'they', 'were.', 'untill', 'yertle', 'the', 'king', 'of', 'them', 'all', 'decided', 'the', 'kingdom', 'he', 'ruled', 'was', 'too', 'small.', 'im', 'ruler', 'said', 'yertle', 'of', 'all', 'that', 'i', 'see.', 'but', 'i', 'dont', 'see', 'enough.', 'thats', 'the', 'trouble', 'with', 'me.', 'with', 'this', 'stone', 'for', 'a', 'throne', 'i', 'look', 'down', 'on', 'my', 'pond', 'but', 'i', 'cannot', 'look', 'down', 'on', 'the', 'places', 'beyond.', 'this', 'throne', 'that', 'i', 'sit', 'on', 'is', 'too', 'too', 'low', 'down.', 'it', 'ought', 'to', 'be', 'higher.', 'he', 'said', 'with', 'a', 'frown.', 'if', 'i', 'could', 'sit', 'high', 'how', 'much', 'greater', 'id', 'be.', 'what', 'a', 'king.', 'id', 'be', 'ruler', 'of', 'all', 'that', 'i', 'see.', 'so', 'yertle', 'the', 'turtle', 'king', 'lifted', 'his', 'hand', 'and', 'yertle', 'the', 'turtle', 'king', 'gave', 'a', 'command.', 'he', 'ordered', 'nine', 'turtles', 'to', 'swim', 'to', 'his', 'stone', 'and', 'using', 'these', 'turtles', 'he', 'built', 'a', 'new', 'throne.', 'he', 'made', 'each', 'turtle', 'stand', 'on', 'another', 'ones', 'back', 'and', 'he', 'piled', 'them']\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = 'data/combinedText.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'big', 'a,', 'little', 'a,', 'what', 'begins', 'with', 'a.', 'aunt', \"annie's\", 'alligator.a.a.a', 'big', 'b,', 'little', 'b,', 'what', 'begins', 'with', 'b.', 'barber', 'baby', 'bubbles', 'and', 'a', 'bumblebee.', 'big', 'c,', 'little', 'c,', 'what', 'begins', 'with', 'c.', 'camel', 'on', 'the', 'ceiling', 'c.c.c', 'big', 'd,', 'little', 'd,', 'what', 'begins', 'with', 'd.', 'david', 'donald', 'doo', 'dreamed', 'a', 'doen', 'doughnuts', 'and', 'a', 'duck-dog,', 'too.', 'abcde.e.e', 'ear', 'egg', 'elephant', 'e', 'e', 'e', 'big', 'f,', 'little', 'f', 'f.', 'f.f', 'four', 'fluffy', 'feathers', 'on', 'a', 'fiffer-feffer-feff.', 'abcdeg', 'goat', 'girl', 'googoo', 'goggles', 'g.g.g', 'big', 'h,', 'little', 'h', 'hungry', 'horse.', 'hay.', 'hen', 'in', 'a', 'hat.', 'hooray.', 'hooray.', 'big', 'i,', 'little', 'i,', 'i.i.i', 'icabod', 'is', 'itchy.', 'so', 'am', 'i.', 'big', 'j,', 'little', 'j,', 'what', 'begins', 'with', 'j.', 'jerry', 'jordans', 'jelly', 'jar', 'and', 'jam', 'begins', 'that', 'way.', 'big', 'k,', 'little', 'k', 'kitten.', 'kangaroo.', 'kick', 'a', 'ketlle.', 'kite', 'and', 'a', \"king's\", 'kerchoo.', 'big', 'l,', 'little', 'l', 'little', 'lola', 'lopp.', 'left', 'leg.', 'lazy', 'lion', 'licks', 'a', 'lollipop.', 'big', 'm,', 'little', 'm', 'many', 'mumbling', 'mice', 'are', 'making', 'midnight', 'music', 'in', 'the', 'moonlight.mighty', 'nice', 'big', 'n,', 'little', 'n,', 'what', 'begins', 'with', 'n.', 'nine', 'new', 'neckties', 'and', 'a', 'nigthshirt', 'and', 'a', 'nose.', 'o', 'is', 'very', 'useful.', 'you', 'use', 'it', 'when', 'you', 'say:', 'oscar;s', 'only', 'ostrich', 'oiled', 'an', 'orange']\n",
      "Total Tokens: 11034\n",
      "Unique Tokens: 2342\n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2636787"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 0\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = 'republic_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "len(sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            79800     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1596)              161196    \n",
      "=================================================================\n",
      "Total params: 391,896\n",
      "Trainable params: 391,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10793/10793 [==============================] - 21s 2ms/step - loss: 6.4315 - acc: 0.0363\n",
      "Epoch 2/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 6.0330 - acc: 0.0406\n",
      "Epoch 3/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 6.0007 - acc: 0.0412\n",
      "Epoch 4/100\n",
      "10793/10793 [==============================] - 23s 2ms/step - loss: 5.8764 - acc: 0.0411\n",
      "Epoch 5/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 5.7949 - acc: 0.0437\n",
      "Epoch 6/100\n",
      "10793/10793 [==============================] - 21s 2ms/step - loss: 5.6893 - acc: 0.0500\n",
      "Epoch 7/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 5.5542 - acc: 0.0574\n",
      "Epoch 8/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 5.4106 - acc: 0.0677\n",
      "Epoch 9/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 5.2841 - acc: 0.0735\n",
      "Epoch 10/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 5.1745 - acc: 0.0782\n",
      "Epoch 11/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 5.0856 - acc: 0.0825\n",
      "Epoch 12/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 5.0043 - acc: 0.0852\n",
      "Epoch 13/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 4.9229 - acc: 0.0925\n",
      "Epoch 14/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 4.8441 - acc: 0.0979\n",
      "Epoch 15/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 4.7696 - acc: 0.1042\n",
      "Epoch 16/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 4.6946 - acc: 0.1085\n",
      "Epoch 17/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 4.6266 - acc: 0.1117\n",
      "Epoch 18/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 4.5628 - acc: 0.1127\n",
      "Epoch 19/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 4.5056 - acc: 0.1166\n",
      "Epoch 20/100\n",
      "10793/10793 [==============================] - 120s 11ms/step - loss: 4.4426 - acc: 0.1215\n",
      "Epoch 21/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 4.3786 - acc: 0.1252\n",
      "Epoch 22/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 4.3170 - acc: 0.1280\n",
      "Epoch 23/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 4.2510 - acc: 0.1359\n",
      "Epoch 24/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 4.1911 - acc: 0.1364\n",
      "Epoch 25/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 4.1371 - acc: 0.1439\n",
      "Epoch 26/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 4.0621 - acc: 0.1492\n",
      "Epoch 27/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 4.0060 - acc: 0.1521\n",
      "Epoch 28/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.9368 - acc: 0.1591\n",
      "Epoch 29/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.8871 - acc: 0.1660\n",
      "Epoch 30/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.8260 - acc: 0.1692\n",
      "Epoch 31/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.7672 - acc: 0.1720\n",
      "Epoch 32/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.7153 - acc: 0.1813\n",
      "Epoch 33/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.6573 - acc: 0.1862\n",
      "Epoch 34/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.6051 - acc: 0.1972\n",
      "Epoch 35/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.5567 - acc: 0.2048\n",
      "Epoch 36/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.5008 - acc: 0.2119\n",
      "Epoch 37/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.4625 - acc: 0.2142\n",
      "Epoch 38/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.3962 - acc: 0.2278\n",
      "Epoch 39/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.3451 - acc: 0.2352\n",
      "Epoch 40/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.2974 - acc: 0.2477\n",
      "Epoch 41/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.2475 - acc: 0.2507\n",
      "Epoch 42/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.1988 - acc: 0.2608\n",
      "Epoch 43/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.1534 - acc: 0.2639\n",
      "Epoch 44/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.0978 - acc: 0.2788\n",
      "Epoch 45/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.0423 - acc: 0.2839\n",
      "Epoch 46/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.9991 - acc: 0.2925\n",
      "Epoch 47/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.9482 - acc: 0.3022\n",
      "Epoch 48/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.9065 - acc: 0.3095\n",
      "Epoch 49/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.8676 - acc: 0.3172\n",
      "Epoch 50/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.8205 - acc: 0.3266\n",
      "Epoch 51/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.7746 - acc: 0.3312\n",
      "Epoch 52/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 2.7239 - acc: 0.3442\n",
      "Epoch 53/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.6828 - acc: 0.3549\n",
      "Epoch 54/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.6488 - acc: 0.3587\n",
      "Epoch 55/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 2.6047 - acc: 0.3691\n",
      "Epoch 56/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.5667 - acc: 0.3751\n",
      "Epoch 57/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.5248 - acc: 0.3796\n",
      "Epoch 58/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.4850 - acc: 0.3924\n",
      "Epoch 59/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.4405 - acc: 0.4013\n",
      "Epoch 60/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 2.4105 - acc: 0.4059\n",
      "Epoch 61/100\n",
      "10793/10793 [==============================] - 25s 2ms/step - loss: 2.3664 - acc: 0.4161\n",
      "Epoch 62/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 2.3339 - acc: 0.4231\n",
      "Epoch 63/100\n",
      "10793/10793 [==============================] - 21s 2ms/step - loss: 2.3034 - acc: 0.4327\n",
      "Epoch 64/100\n",
      "10793/10793 [==============================] - 22s 2ms/step - loss: 2.2633 - acc: 0.4384\n",
      "Epoch 65/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.2364 - acc: 0.4451\n",
      "Epoch 66/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.2046 - acc: 0.4482\n",
      "Epoch 67/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.1639 - acc: 0.4610\n",
      "Epoch 68/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.1340 - acc: 0.4677\n",
      "Epoch 69/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.1025 - acc: 0.4781\n",
      "Epoch 70/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.0745 - acc: 0.4792\n",
      "Epoch 71/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.0502 - acc: 0.4877\n",
      "Epoch 72/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.0129 - acc: 0.4970\n",
      "Epoch 73/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.9794 - acc: 0.5033\n",
      "Epoch 74/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.9501 - acc: 0.5115\n",
      "Epoch 75/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.9255 - acc: 0.5164\n",
      "Epoch 76/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.8943 - acc: 0.5227\n",
      "Epoch 77/100\n",
      "10793/10793 [==============================] - 22s 2ms/step - loss: 1.8702 - acc: 0.5282\n",
      "Epoch 78/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.8464 - acc: 0.5318\n",
      "Epoch 79/100\n",
      "10793/10793 [==============================] - 21s 2ms/step - loss: 1.8125 - acc: 0.5499\n",
      "Epoch 80/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.7937 - acc: 0.5498\n",
      "Epoch 81/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 1.7633 - acc: 0.5560\n",
      "Epoch 82/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 1.7285 - acc: 0.5648\n",
      "Epoch 83/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 1.7038 - acc: 0.5671\n",
      "Epoch 84/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 1.6848 - acc: 0.5695\n",
      "Epoch 85/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 1.6606 - acc: 0.5801\n",
      "Epoch 86/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.6271 - acc: 0.5881\n",
      "Epoch 87/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.6174 - acc: 0.5921\n",
      "Epoch 88/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.6044 - acc: 0.5895\n",
      "Epoch 89/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 1.5700 - acc: 0.6000\n",
      "Epoch 90/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.5434 - acc: 0.6059\n",
      "Epoch 91/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.5226 - acc: 0.6058\n",
      "Epoch 92/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.5050 - acc: 0.6158\n",
      "Epoch 93/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.4919 - acc: 0.6173\n",
      "Epoch 94/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 1.4486 - acc: 0.6302\n",
      "Epoch 95/100\n",
      "10793/10793 [==============================] - 23s 2ms/step - loss: 1.4330 - acc: 0.6369\n",
      "Epoch 96/100\n",
      "10793/10793 [==============================] - 22s 2ms/step - loss: 1.4131 - acc: 0.6386\n",
      "Epoch 97/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 1.3833 - acc: 0.6451\n",
      "Epoch 98/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.3681 - acc: 0.6550\n",
      "Epoch 99/100\n",
      "10793/10793 [==============================] - 23s 2ms/step - loss: 1.3350 - acc: 0.6594\n",
      "Epoch 100/100\n",
      "10793/10793 [==============================] - 24s 2ms/step - loss: 1.3194 - acc: 0.6648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1824643908>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model('model.h5')\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whosever room this is should be ashamed!\n",
      "His underwear is hanging on the lamp.\n",
      "His raincoat is there in the overstuffed chair,\n",
      "And the chair is becoming quite mucky and damp.\n",
      "His workbook is wedged in the window,\n",
      "His sweater's been thrown on the floor.\n",
      "His scarf and one ski are\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "# seed_text = lines[randint(0,len(lines))]\n",
    "seed_text = '''Whosever room this is should be ashamed!\n",
    "His underwear is hanging on the lamp.\n",
    "His raincoat is there in the overstuffed chair,\n",
    "And the chair is becoming quite mucky and damp.\n",
    "His workbook is wedged in the window,\n",
    "His sweater's been thrown on the floor.\n",
    "His scarf and one ski are'''\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youre but whoville without clover by bipping their voices hour their snoots and the air murmured horton never thought all he laughed as their shoes but not started to haul him into a beezlenut almost an called an bags and floors saved all whoville ruckus and roar they do sniff\n"
     ]
    }
   ],
   "source": [
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
