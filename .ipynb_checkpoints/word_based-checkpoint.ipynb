{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from pickle import dump\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import sys\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# load document\n",
    "drseuss_text = 'data/combinedText.txt'\n",
    "tokens = load_doc(drseuss_text)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- PARAMETERS --- --- --- ---- --- --- ---- ---- --- ----- --- --- ----\n",
    "#--- --- ---- --- --- --- --- ---- --- --- --- ----- ---- ---- ---- ---- -\n",
    "drseuss_text = 'data/combinedText.txt'\n",
    "seed_length = 50\n",
    "length = seed_length + 1\n",
    "epochs = 500\n",
    "batch_size=128\n",
    "#--- --- ---- --- --- --- --- ---- --- --- --- ----- ---- ---- ---- ---- -\n",
    "#--- --- ---- --- --- --- --- ---- --- --- --- ----- ---- ---- ---- ---- -\n",
    "\n",
    "# organize into sequences of tokens\n",
    "#the plus one is because the last val in the list will be the expected prediction. \n",
    "#Its our Y-train\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    #line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(seq)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "print(f'sequences: {type(sequences[0])}')\n",
    "\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame(sequences)\n",
    "# X = df.iloc[:,:-1]\n",
    "# y = df.iloc[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# integer encode sequences of words\n",
    "#sequences = [str(i) for i in sequences]\n",
    "# print(f'tokenizer: {tokenizer}')\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "# print(f'tokenizer: {tokenizer}')\n",
    "sequences = tokenizer.texts_to_sequences(sequences)\n",
    "# print(f'sequences: {sequences}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- PARAMETERS -- ---- --- ---- --- --- ---- --- ---- --- ---- --- ---- ---\n",
    "#-- ---- ---- --- ---- ----- ---- ----- ---- ----- ----- ---- ---- ---- ----\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "modelList = [('LSTM',256,'True'), ('Dense',256,'relu'), ('Dropout',.2,''), \n",
    "             ('LSTM',128,'True'), ('Dense',128,'relu'), ('Dropout',.2,''), \n",
    "             ('LSTM', 64,'False'), ('Dense',64,'relu'), \n",
    "             ('Flatten','',''),('Dense',vocab_size,'softmax')]\n",
    "\n",
    "#notes from website:\n",
    "#-- Common values are 50, 100, and 300. We will use 50 here, --\n",
    "#-- but consider testing smaller or larger values. --\n",
    "#-- We will use a two LSTM hidden layers with 100 memory cells each. --\n",
    "#-- More memory cells and a deeper network may achieve better results. --\n",
    "#-- ---- ---- --- ---- ----- ---- ----- ---- ----- ----- ---- ---- ---- ----\n",
    "#-- ---- ---- --- ---- ----- ---- ----- ---- ----- ----- ---- ---- ---- ----\n",
    "\n",
    "#Create the model name\n",
    "modelName = f'm_{length}'\n",
    "for layer in modelList:\n",
    "    modelName+= f'_{layer[0]}_{layer[1]}_{layer[2]}'\n",
    "modelName += '.h5'\n",
    "modelName\n",
    "\n",
    "#create tokenizer file name .pkl\n",
    "tokenizerName = 'toke' + modelName.replace('m','',1).split('.h5')[0] + '.pkl'\n",
    "print(f'drseuss_text: \\'{drseuss_text}\\'\\nseed_length: {seed_length}\\nepochs: {epochs}\\nbatch_size: {batch_size}'\n",
    "     f'\\nmodelName: {modelName}\\ntokenizerName: {tokenizerName}\\nmodelList: {modelList}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(sequences)\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "print(f'seq_length: {seq_length}\\nshape of X: {X.shape}\\nshape of y: {y.shape}')\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, seq_length, input_length=seq_length))\n",
    "print(f'model.add(Embedding({vocab_size}, {seq_length}, input_length={seq_length}))')\n",
    "for layer in modelList:\n",
    "    if layer[0] == 'LSTM':\n",
    "        #model.add(LSTM(100, return_sequences=True))\n",
    "        (_, neurons, rsequences) = layer\n",
    "        model.add(LSTM(neurons, return_sequences=rsequences))\n",
    "        print(f'model.add(LSTM({neurons}, return_sequences={rsequences}))')\n",
    "        \n",
    "    if layer[0] == 'Dropout':\n",
    "        #model.add(Dropout(0.2))\n",
    "        (_, dropout_rate, _) = layer\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        print(f'model.add(Dropout({dropout_rate}))')\n",
    "        \n",
    "    if layer[0] == 'Dense':\n",
    "        #model.add(Dense(100, activation='relu'))\n",
    "        (_, neurons, afunction) = layer\n",
    "        model.add(Dense(neurons, activation=afunction))\n",
    "        print(f'model.add(Dense({neurons}, activation={afunction}))')\n",
    "        \n",
    "    if layer[0] == 'Flatten':\n",
    "        model.add(Flatten())\n",
    "        print(f'model.add(Flatten())')\n",
    "        \n",
    "#model.add(LSTM(100, return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(LSTM(100))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "modelName = f'm_{length}'\n",
    "for layer in modelList:\n",
    "    modelName+= f'_{layer[0]}_{layer[1]}_{layer[2]}'\n",
    "modelName += '.h5'\n",
    "\n",
    "# save the model to file\n",
    "model.save(modelName)\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open(tokenizerName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "#def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model(modelName)\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = load(open(tokenizerName, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a seed text\n",
    "# seed_text = lines[randint(0,len(lines))]\n",
    "seed_text = '''Whosever room this is should be ashamed!\n",
    "His underwear is hanging on the lamp.\n",
    "His raincoat is there in the overstuffed chair,\n",
    "And the chair is becoming quite mucky and damp.\n",
    "His workbook is wedged in the window,\n",
    "His sweater's been thrown on the floor.\n",
    "His scarf and one ski are'''\n",
    "seed_text = ' '.join(seed_text.split(' ')[0:50])\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode our seed\n",
    "encoded = tokenizer.texts_to_sequences([seed_text])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, seed_length)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
