{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'e', 't', 'o', 'a', 'h', 'n', 'i', 's', 'l', 'r', 'd', '.', 'u', 'w', 'y', 'm', 'c', 'g', 'b', 'k', 'p', 'f', 'v', \"'\", 'x', '’', 'j', '“', '”', 'z', 'q', '…', ':', ';', '1', '5', '6', '2', '9', '8', '3', '/', '4']\n",
      "{' ': 0, 'e': 1, 't': 2, 'o': 3, 'a': 4, 'h': 5, 'n': 6, 'i': 7, 's': 8, 'l': 9, 'r': 10, 'd': 11, '.': 12, 'u': 13, 'w': 14, 'y': 15, 'm': 16, 'c': 17, 'g': 18, 'b': 19, 'k': 20, 'p': 21, 'f': 22, 'v': 23, \"'\": 24, 'x': 25, '’': 26, 'j': 27, '“': 28, '”': 29, 'z': 30, 'q': 31, '…': 32, ':': 33, ';': 34, '1': 35, '5': 36, '6': 37, '2': 38, '9': 39, '8': 40, '3': 41, '/': 42, '4': 43} {0: ' ', 1: 'e', 2: 't', 3: 'o', 4: 'a', 5: 'h', 6: 'n', 7: 'i', 8: 's', 9: 'l', 10: 'r', 11: 'd', 12: '.', 13: 'u', 14: 'w', 15: 'y', 16: 'm', 17: 'c', 18: 'g', 19: 'b', 20: 'k', 21: 'p', 22: 'f', 23: 'v', 24: \"'\", 25: 'x', 26: '’', 27: 'j', 28: '“', 29: '”', 30: 'z', 31: 'q', 32: '…', 33: ':', 34: ';', 35: '1', 36: '5', 37: '6', 38: '2', 39: '9', 40: '8', 41: '3', 42: '/', 43: '4'}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "with open(os.path.join('data','combinedText.txt'), encoding=\"utf-8\", errors='ignore') as f:\n",
    "    words = ' '.join(f.readlines())\n",
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    print([c[0] for c in count])\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "d1, d2 = build_dataset(words)\n",
    "print(d1, d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "# Input data - binary numbers for each integer from 0 to 256\n",
    "int_to_binary = {}\n",
    "binary_dim = 8\n",
    "max_val = (2**binary_dim) #2^8 = 256\n",
    "binary_val = np.unpackbits(np.array([range(max_val)], dtype=np.uint8).T, axis=1) # Calc Binary values for ints 0-256\n",
    "for i in range(max_val): # map Integer values to Binary values\n",
    "    int_to_binary[i] = binary_val[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RecurrentNeuralNetwork:\n",
    "    #input (word), expected output (next word), num of words (num of recurrences), array expected outputs, learning rate\n",
    "    def __init__ (self, xs, ys, rl, eo, lr):\n",
    "        #initial input (first word)\n",
    "        self.x = np.zeros(xs)\n",
    "        #input size \n",
    "        self.xs = xs\n",
    "        #expected output (next word)\n",
    "        self.y = np.zeros(ys)\n",
    "        #output size\n",
    "        self.ys = ys\n",
    "        #weight matrix for interpreting results from LSTM cell (num words x num words matrix)\n",
    "        self.w = np.random.random((ys, ys))\n",
    "        #matrix used in RMSprop\n",
    "        self.G = np.zeros_like(self.w)\n",
    "        #length of the recurrent network - number of recurrences i.e num of words\n",
    "        self.rl = rl\n",
    "        #learning rate \n",
    "        self.lr = lr\n",
    "        #array for storing inputs\n",
    "        self.ia = np.zeros((rl+1,xs))\n",
    "        #array for storing cell states\n",
    "        self.ca = np.zeros((rl+1,ys))\n",
    "        #array for storing outputs\n",
    "        self.oa = np.zeros((rl+1,ys))\n",
    "        #array for storing hidden states\n",
    "        self.ha = np.zeros((rl+1,ys))\n",
    "        #forget gate \n",
    "        self.af = np.zeros((rl+1,ys))\n",
    "        #input gate\n",
    "        self.ai = np.zeros((rl+1,ys))\n",
    "        #cell state\n",
    "        self.ac = np.zeros((rl+1,ys))\n",
    "        #output gate\n",
    "        self.ao = np.zeros((rl+1,ys))\n",
    "        #array of expected output values\n",
    "        self.eo = np.vstack((np.zeros(eo.shape[0]), eo.T))\n",
    "        #declare LSTM cell (input, output, amount of recurrence, learning rate)\n",
    "        self.LSTM = LSTM(xs, ys, rl, lr)\n",
    "    \n",
    "    #activation function. simple nonlinearity, convert nums into probabilities between 0 and 1\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    #the derivative of the sigmoid function. used to compute gradients for backpropagation\n",
    "    def dsigmoid(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))    \n",
    "    \n",
    "    #lets apply a series of matrix operations to our input (curr word) to compute a predicted output (next word)\n",
    "    def forwardProp(self):\n",
    "        for i in range(1, self.rl+1):\n",
    "            self.LSTM.x = np.hstack((self.ha[i-1], self.x))\n",
    "            cs, hs, f, c, o = self.LSTM.forwardProp()\n",
    "            #store computed cell state\n",
    "            self.ca[i] = cs\n",
    "            self.ha[i] = hs\n",
    "            self.af[i] = f\n",
    "            self.ai[i] = inp\n",
    "            self.ac[i] = c\n",
    "            self.ao[i] = o\n",
    "            self.oa[i] = self.sigmoid(np.dot(self.w, hs))\n",
    "            self.x = self.eo[i-1]\n",
    "        return self.oa\n",
    "   \n",
    "    \n",
    "    def backProp(self):\n",
    "        #update our weight matrices (Both in our Recurrent network, as well as the weight matrices inside LSTM cell)\n",
    "        #init an empty error value \n",
    "        totalError = 0\n",
    "        #initialize matrices for gradient updates\n",
    "        #First, these are RNN level gradients\n",
    "        #cell state\n",
    "        dfcs = np.zeros(self.ys)\n",
    "        #hidden state,\n",
    "        dfhs = np.zeros(self.ys)\n",
    "        #weight matrix\n",
    "        tu = np.zeros((self.ys,self.ys))\n",
    "        #Next, these are LSTM level gradients\n",
    "        #forget gate\n",
    "        tfu = np.zeros((self.ys, self.xs+self.ys))\n",
    "        #input gate\n",
    "        tiu = np.zeros((self.ys, self.xs+self.ys))\n",
    "        #cell unit\n",
    "        tcu = np.zeros((self.ys, self.xs+self.ys))\n",
    "        #output gate\n",
    "        tou = np.zeros((self.ys, self.xs+self.ys))\n",
    "        #loop backwards through recurrences\n",
    "        for i in range(self.rl, -1, -1):\n",
    "            #error = calculatedOutput - expectedOutput\n",
    "            error = self.oa[i] - self.eo[i]\n",
    "            #calculate update for weight matrix\n",
    "            #(error * derivative of the output) * hidden state\n",
    "            tu += np.dot(np.atleast_2d(error * self.dsigmoid(self.oa[i])), np.atleast_2d(self.ha[i]).T)\n",
    "            #Time to propagate error back to exit of LSTM cell\n",
    "            #1. error * RNN weight matrix\n",
    "            error = np.dot(error, self.w)\n",
    "            #2. set input values of LSTM cell for recurrence i (horizontal stack of arrays, hidden + input)\n",
    "            self.LSTM.x = np.hstack((self.ha[i-1], self.ia[i]))\n",
    "            #3. set cell state of LSTM cell for recurrence i (pre-updates)\n",
    "            self.LSTM.cs = self.ca[i]\n",
    "            #Finally, call the LSTM cell's backprop, retreive gradient updates\n",
    "            #gradient updates for forget, input, cell unit, and output gates + cell states & hiddens states\n",
    "            fu, iu, cu, ou, dfcs, dfhs = self.LSTM.backProp(error, self.ca[i-1], self.af[i], self.ai[i], self.ac[i], self.ao[i], dfcs, dfhs)\n",
    "            #calculate total error (not necesarry, used to measure training progress)\n",
    "            totalError += np.sum(error)\n",
    "            #accumulate all gradient updates\n",
    "            #forget gate\n",
    "            tfu += fu\n",
    "            #input gate\n",
    "            tiu += iu\n",
    "            #cell state\n",
    "            tcu += cu\n",
    "            #output gate\n",
    "            tou += ou\n",
    "        #update LSTM matrices with average of accumulated gradient updates    \n",
    "        self.LSTM.update(tfu/self.rl, tiu/self.rl, tcu/self.rl, tou/self.rl) \n",
    "        #update weight matrix with average of accumulated gradient updates  \n",
    "        self.update(tu/self.rl)\n",
    "        #return total error of this iteration\n",
    "        return totalError\n",
    "    \n",
    "    def update(self, u):\n",
    "        #vanilla implementation of RMSprop\n",
    "        self.G = 0.9 * self.G + 0.1 * u**2  \n",
    "        self.w -= self.lr/np.sqrt(self.G + 1e-8) * u\n",
    "        return\n",
    "    \n",
    "    #this is where we generate some sample text after having fully trained our model\n",
    "    #i.e error is below some threshold\n",
    "    def sample(self):\n",
    "         #loop through recurrences - start at 1 so the 0th entry of all arrays will be an array of 0's\n",
    "        for i in range(1, self.rl+1):\n",
    "            #set input for LSTM cell, combination of input (previous output) and previous hidden state\n",
    "            self.LSTM.x = np.hstack((self.ha[i-1], self.x))\n",
    "            #run forward prop on the LSTM cell, retrieve cell state and hidden state\n",
    "            cs, hs, f, inp, c, o = self.LSTM.forwardProp()\n",
    "            #store input as vector\n",
    "            maxI = np.argmax(self.x)\n",
    "            self.x = np.zeros_like(self.x)\n",
    "            self.x[maxI] = 1\n",
    "            self.ia[i] = self.x #Use np.argmax?\n",
    "            #store cell states\n",
    "            self.ca[i] = cs\n",
    "            #store hidden state\n",
    "            self.ha[i] = hs\n",
    "            #forget gate\n",
    "            self.af[i] = f\n",
    "            #input gate\n",
    "            self.ai[i] = inp\n",
    "            #cell state\n",
    "            self.ac[i] = c\n",
    "            #output gate\n",
    "            self.ao[i] = o\n",
    "            #calculate output by multiplying hidden state with weight matrix\n",
    "            self.oa[i] = self.sigmoid(np.dot(self.w, hs))\n",
    "            #compute new input\n",
    "            maxI = np.argmax(self.oa[i])\n",
    "            newX = np.zeros_like(self.x)\n",
    "            newX[maxI] = 1\n",
    "            self.x = newX\n",
    "        #return all outputs    \n",
    "        return self.oa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
