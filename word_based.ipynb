{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy, sys, pandas as pd\n",
    "from random import randint\n",
    "from pickle import dump, load\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, Flatten\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    tokens = text.split()\n",
    "    print(tokens[:100])\n",
    "    print('Total Tokens: %d' % len(tokens))\n",
    "    print('Unique Tokens: %d' % len(set(tokens)))\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# organize into sequences of tokens\n",
    "#the plus one is because the last val in the list will be the expected prediction. \n",
    "#Its our Y-train\n",
    "def sequencesCreate(length, tokens):\n",
    "    sequences = list()\n",
    "    for i in range(length, len(tokens)):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[i-length:i]\n",
    "        # convert into a line\n",
    "        #line = ' '.join(seq)\n",
    "        # store\n",
    "        sequences.append(seq)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    print(f'sequences: {type(sequences[0])}')\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    # integer encode sequences of words\n",
    "    #sequences = [str(i) for i in sequences]\n",
    "    # print(f'tokenizer: {tokenizer}')\n",
    "    tokenizer.fit_on_texts(sequences)\n",
    "    # print(f'tokenizer: {tokenizer}')\n",
    "    sequences = tokenizer.texts_to_sequences(sequences)\n",
    "    # print(f'sequences: {sequences}')\n",
    "    \n",
    "    return sequences, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "def defineModel(vocab_size, seq_length, modelList, length):\n",
    "    model = Sequential()\n",
    "    print(f'model.add(Embedding({vocab_size}, {seq_length}, input_length={seq_length}))')\n",
    "    for layer in modelList:\n",
    "        if layer[0] == 'Embedding': \n",
    "            (_, i2, i3) = layer\n",
    "            model.add(Embedding(i2, i3, input_length=i3))\n",
    "        if layer[0] == 'LSTM':\n",
    "            #model.add(LSTM(100, return_sequences=True))\n",
    "            (_, neurons, rsequences) = layer\n",
    "            model.add(LSTM(neurons, return_sequences=rsequences))\n",
    "            print(f'model.add(LSTM({neurons}, return_sequences={rsequences}))')\n",
    "\n",
    "        if layer[0] == 'Dropout':\n",
    "            #model.add(Dropout(0.2))\n",
    "            (_, dropout_rate, _) = layer\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            print(f'model.add(Dropout({dropout_rate}))')\n",
    "\n",
    "        if layer[0] == 'Dense':\n",
    "            #model.add(Dense(100, activation='relu'))\n",
    "            (_, neurons, afunction) = layer\n",
    "            model.add(Dense(neurons, activation=afunction))\n",
    "            print(f'model.add(Dense({neurons}, activation={afunction}))')\n",
    "\n",
    "        if layer[0] == 'Flatten':\n",
    "            model.add(Flatten())\n",
    "            print(f'model.add(Flatten())')\n",
    "        \n",
    "    #Create the model name\n",
    "    modelName = f'{length}'\n",
    "    for layer in modelList:\n",
    "        modelName+= f'_{layer[0]}_{layer[1]}_{layer[2]}'\n",
    "\n",
    "    #model.add(LSTM(100, return_sequences=True))\n",
    "    #model.add(Dropout(0.2))\n",
    "    #model.add(LSTM(100))\n",
    "    #model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dense(vocab_size, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    return model, modelName\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelFit(model, modelName, X, y, seq_length, batch_size, epochs):\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # define the checkpoint\n",
    "    filepath=f\"wi_{{epoch:02d}}_{{loss:.4f}}__{modelName}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    # fit model\n",
    "    history_callback = model.fit(X, y, batch_size=batch_size, epochs=epochs, callbacks=callbacks_list)\n",
    "    return history_callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- --- ---- --- ---- --- ---- ---- --- ----- ---- ---\n",
    "# -- Write Files ---- ---- ---- --- ---- --- --- --- -- \n",
    "#--- --- ---- --- ---- --- ---- ---- --- ----- ---- ---\n",
    "def writeFiles(modelName, history_callback):\n",
    "    loss_history = history_callback.history\n",
    "    \n",
    "    # save the model to file\n",
    "    model.save('m_' + modelName + '.h5')\n",
    "\n",
    "    # save losses\n",
    "    with open('h_' + modelName + '.txt', 'w+') as f:\n",
    "        f.write(str(loss_history))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whosever room this is should be ashamed!\n",
      "His underwear is hanging on the lamp.\n",
      "His raincoat is there in the overstuffed chair,\n",
      "And the chair is becoming quite mucky and damp.\n",
      "His workbook is wedged in the window,\n",
      "His sweater's been thrown on the floor.\n",
      "His scarf and one ski are\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "# seed_text = lines[randint(0,len(lines))]\n",
    "seed_text = '''Whosever room this is should be ashamed!\n",
    "His underwear is hanging on the lamp.\n",
    "His raincoat is there in the overstuffed chair,\n",
    "And the chair is becoming quite mucky and damp.\n",
    "His workbook is wedged in the window,\n",
    "His sweater's been thrown on the floor.\n",
    "His scarf and one ski are'''\n",
    "\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "#def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "def generate_seq(modelName, tokenizerName, seq_length, seed_text, n_words):\n",
    "    # load the model\n",
    "    model = load_model(modelName)\n",
    "\n",
    "    # load the tokenizer\n",
    "    tokenizer = load(open(tokenizerName, 'rb'))\n",
    "    \n",
    "    #Make 50 words long\n",
    "    seed_text = ' '.join(seed_text.split(' ')[0:seq_length])\n",
    "    \n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModelComplete():\n",
    "    # -- PARAMETERS -- ---- --- ---- --- --- ---- --- ---- --- ---- --- ---- ---\n",
    "    #-- ---- ---- --- ---- ----- ---- ----- ---- ----- ----- ---- ---- ---- ----\n",
    "    #--- PARAMETERS --- --- --- ---- --- --- ---- ---- --- ----- --- --- ----\n",
    "    #--- --- ---- --- --- --- --- ---- --- --- --- ----- ---- ---- ---- ---- -\n",
    "    drseuss_text = 'data/combinedText.txt'\n",
    "    seed_length = 50\n",
    "    length = seed_length + 1\n",
    "    epochs = 2\n",
    "    batch_size = 128\n",
    "    #--- --- ---- --- --- --- --- ---- --- --- --- ----- ---- ---- ---- ---- -\n",
    "    #--- --- ---- --- --- --- --- ---- --- --- --- ----- ---- ---- ---- ---- -\n",
    "\n",
    "    #notes from website:\n",
    "    #-- Common values are 50, 100, and 300. We will use 50 here, --\n",
    "    #-- but consider testing smaller or larger values. --\n",
    "    #-- We will use a two LSTM hidden layers with 100 memory cells each. --\n",
    "    #-- More memory cells and a deeper network may achieve better results. --\n",
    "    #-- ---- ---- --- ---- ----- ---- ----- ---- ----- ----- ---- ---- ---- ----\n",
    "    #-- ---- ---- --- ---- ----- ---- ----- ---- ----- ----- ---- ---- ---- ----\n",
    "    \n",
    "    # load document\n",
    "    drseuss_text = 'data/combinedText.txt'\n",
    "    tokens = load_doc(drseuss_text)\n",
    "\n",
    "    sequences, tokenizer = sequencesCreate(length, tokens)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    df = pd.DataFrame(sequences)\n",
    "    X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "    #One hot encoding\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "    seq_length = X.shape[1]\n",
    "    print(f'seq_length: {seq_length}\\nshape of X: {X.shape}\\nshape of y: {y.shape}')\n",
    "    print(y[0])\n",
    "    \n",
    "\n",
    "    modelList = [('Embedding', vocab_size, seq_length), ('LSTM',256,'True'), ('Dense',256,'relu'), ('Dropout',.2,''), \n",
    "                 ('LSTM',128,'True'), ('Dense',128,'relu'), ('Dropout',.2,''), \n",
    "                 ('LSTM', 64,'False'), ('Dense',64,'relu'), \n",
    "                 ('Flatten','',''),('Dense',vocab_size,'softmax')]\n",
    "    print(f'drseuss_text: \\'{drseuss_text}\\'\\nseed_length: {seed_length}\\nepochs: {epochs}\\nbatch_size: {batch_size}'\n",
    "     f'\\nmodelList: {modelList}')\n",
    "    #oneThing = defineModel(vocab_size, seq_length, modelList)\n",
    "    #print(oneThing)\n",
    "    model, modelName = defineModel(vocab_size, seq_length, modelList, length)\n",
    "    #create tokenizer file name .pkl\n",
    "    tokenizerName = 'toke_' + modelName + '.pkl'\n",
    "    # save the tokenizer\n",
    "    dump(tokenizer, open(tokenizerName, 'wb'))\n",
    "    \n",
    "    \n",
    "    history_callback = modelFit(model, modelName, X, y, seq_length, batch_size, epochs)\n",
    "    writeFiles(modelName, history_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'm_51_LSTM_256_True_Dense_256_relu_Dropout_0.2__LSTM_128_True_Dense_128_relu_Dropout_0.2__LSTM_64_False_Dense_64_relu_Flatten___Dense_2830_softmax.h5'\n",
    "tokenizer = 'toke_51_LSTM_256_True_Dense_256_relu_Dropout_0.2__LSTM_128_True_Dense_128_relu_Dropout_0.2__LSTM_64_False_Dense_64_relu_Flatten___Dense_2830_softmax.pkl'\n",
    "\n",
    "# generate new text\n",
    "# generated = generate_seq(model, tokenizer, seq_length, seed_text, seed_length)\n",
    "# print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yertle', 'the', 'turtle', 'on', 'the', 'far', 'away', 'island', 'of', 'sala', 'ma', 'sond', 'yertle', 'the', 'turtle', 'was', 'king', 'of', 'the', 'pond.', 'a', 'nice', 'little', 'pond.', 'it', 'was', 'clean.', 'it', 'was', 'neat.', 'the', 'water', 'was', 'warm.', 'there', 'was', 'plenty', 'to', 'eat.', 'the', 'turtles', 'had', 'everything', 'turtles', 'might', 'need.', 'and', 'they', 'were', 'all', 'happy.', 'quite', 'happy', 'indeed.', 'they', 'were.', 'untill', 'yertle', 'the', 'king', 'of', 'them', 'all', 'decided', 'the', 'kingdom', 'he', 'ruled', 'was', 'too', 'small.', 'im', 'ruler', 'said', 'yertle', 'of', 'all', 'that', 'i', 'see.', 'but', 'i', 'dont', 'see', 'enough.', 'thats', 'the', 'trouble', 'with', 'me.', 'with', 'this', 'stone', 'for', 'a', 'throne', 'i', 'look', 'down', 'on']\n",
      "Total Tokens: 16226\n",
      "Unique Tokens: 2829\n",
      "Total Sequences: 16175\n",
      "sequences: <class 'list'>\n",
      "seq_length: 50\n",
      "shape of X: (16175, 50)\n",
      "shape of y: (16175, 2830)\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "drseuss_text: 'data/combinedText.txt'\n",
      "seed_length: 50\n",
      "epochs: 2\n",
      "batch_size: 128\n",
      "modelList: [('Embedding', 2830, 50), ('LSTM', 256, 'True'), ('Dense', 256, 'relu'), ('Dropout', 0.2, ''), ('LSTM', 128, 'True'), ('Dense', 128, 'relu'), ('Dropout', 0.2, ''), ('LSTM', 64, 'False'), ('Dense', 64, 'relu'), ('Flatten', '', ''), ('Dense', 2830, 'softmax')]\n",
      "model.add(Embedding(2830, 50, input_length=50))\n",
      "model.add(LSTM(256, return_sequences=True))\n",
      "model.add(Dense(256, activation=relu))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(LSTM(128, return_sequences=True))\n",
      "model.add(Dense(128, activation=relu))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(LSTM(64, return_sequences=False))\n",
      "model.add(Dense(64, activation=relu))\n",
      "model.add(Flatten())\n",
      "model.add(Dense(2830, activation=softmax))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            141500    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 256)           314368    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50, 256)           65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 128)           197120    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50, 128)           16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50, 64)            49408     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50, 64)            4160      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2830)              9058830   \n",
      "=================================================================\n",
      "Total params: 9,847,690\n",
      "Trainable params: 9,847,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "  256/16175 [..............................] - ETA: 6:03 - loss: 7.9472 - acc: 0.0039    "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-006cf18cdc2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrainModelComplete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-62684f1ee0c5>\u001b[0m in \u001b[0;36mtrainModelComplete\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mhistory_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelFit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mwriteFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8b6a15c01171>\u001b[0m in \u001b[0;36mmodelFit\u001b[0;34m(model, modelName, X, y, seq_length, batch_size, epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mhistory_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    trainModelComplete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainModelComplete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
