{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, sys, os, pandas as pd\n",
    "from random import randint\n",
    "from pickle import dump, load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    tokens = text.split()\n",
    "    print(tokens[:100])\n",
    "    print('Total Tokens: %d' % len(tokens))\n",
    "    print('Unique Tokens: %d' % len(set(tokens)))\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize into sequences of tokens\n",
    "#the plus one is because the last val in the list will be the expected prediction. \n",
    "#Its our Y-train\n",
    "def sequencesCreate(length, tokens):\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    sequences = list()\n",
    "    for i in range(length, len(tokens)):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[i-length:i]\n",
    "        # convert into a line\n",
    "        #line = ' '.join(seq)\n",
    "        # store\n",
    "        sequences.append(seq)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    print(f'sequences[0][0]: {sequences[0][0]}')\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    # integer encode sequences of words\n",
    "    #sequences = [str(i) for i in sequences]\n",
    "    # print(f'tokenizer: {tokenizer}')\n",
    "    tokenizer.fit_on_texts(sequences)\n",
    "    # print(f'tokenizer: {tokenizer}')\n",
    "    sequences = tokenizer.texts_to_sequences(sequences)\n",
    "    # print(f'sequences: {sequences}')\n",
    "    \n",
    "    return sequences, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelFit(model, modelName, X, y, seq_length, batch_size, epochs, results_path):\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # define the checkpoint\n",
    "    filepath=f\"{results_path.rstrip('/').lstrip('/')}/wi_{{epoch:02d}}_{{loss:.4f}}_{modelName}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    # fit model\n",
    "    history_callback = model.fit(X, y, batch_size=batch_size, epochs=epochs, callbacks=callbacks_list)\n",
    "    return history_callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- --- ---- --- ---- --- ---- ---- --- ----- ---- ---\n",
    "# -- Write Files ---- ---- ---- --- ---- --- --- --- -- \n",
    "#--- --- ---- --- ---- --- ---- ---- --- ----- ---- ---\n",
    "def writeFiles(modelName, modelList, seq_length, total_sequences, epochs, batch_size, results_path):\n",
    "    model_info = {} #history_callback.history\n",
    "    model_info['seq_length'] = seq_length\n",
    "    model_info['total_sequences'] = total_sequences\n",
    "    model_info['batch_size'] = batch_size\n",
    "    model_info['epochs'] = epochs\n",
    "    \n",
    "    # save losses\n",
    "    rFile = results_path.rstrip('/').lstrip('/') + '/info_' + modelName + '.txt'\n",
    "    print(f'Info File: {rFile}')\n",
    "    with open(rFile,'w+') as f:\n",
    "        f.write(str(modelList))\n",
    "        f.write('\\n')\n",
    "        f.write(str(model_info))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "def defineModel(vocab_size, seq_length, modelList, length, input_shape):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import Dropout\n",
    "    from keras.layers import LSTM\n",
    "    from keras.utils import np_utils\n",
    "    from keras.layers import Embedding, Flatten\n",
    "    model = Sequential()\n",
    "    #-- EMBEDDED LAYER --- --- --- ---- --\n",
    "    #input_dim: size of the vocabulary in the text data.\n",
    "    #output_dim: size of the vector space where words will be embedded. or size of the output vectors from this layer try 32 or 100 or larger\n",
    "    #input_length: length of input seq's. ex: if input documents are comprised of 1000 words, it would be 1000.\n",
    "#     modelList = [{'model':'Embedding', 'input_dim':vocab_size, 'output_dim': 100, 'input_length': seq_length},\n",
    "#                  {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': .2}, \n",
    "#                  {'model': 'Dense','units':64,'activation':'relu'}, \n",
    "#                  {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': .2}, \n",
    "#                  {'model': 'Dense','units':64,'activation':'relu'}, \n",
    "#                  {'model':'Flatten'},\n",
    "#                  {'model': 'Dense','units':vocab_size,'activation':'softmax'},\n",
    "#                 ]\n",
    "    for i,layer in enumerate(modelList):\n",
    "        if layer['model'] == 'Embedding': \n",
    "            model.add(Embedding(input_dim=layer['input_dim'], output_dim=layer['output_dim'], \n",
    "                                input_length=layer['input_length']))\n",
    "\n",
    "            print(f\"model.add(Embedding(input_dim= {layer['input_dim']}, output_dim={ layer['output_dim'] }, input_length={ layer['input_length'] }))\")\n",
    "        elif layer['model'] == 'LSTM':\n",
    "            #model.add(LSTM(100, return_sequences=True))\n",
    "            #model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, input_dim=1))\n",
    "            model.add(LSTM(units=layer['units'], use_bias=layer['use_bias'], \n",
    "                           dropout=layer['dropout'], recurrent_dropout=layer['recurrent_dropout'], \n",
    "                           return_sequences = layer['return_sequences']))\n",
    "            print(f\"model.add(LSTM(units={layer['units']}, use_bias={layer['use_bias']}, dropout={layer['dropout']}, recurrent_dropout={layer['recurrent_dropout']} ))\")\n",
    "\n",
    "        elif layer['model'] == 'Dropout':\n",
    "            #model.add(Dropout(0.2))\n",
    "            model.add(Dropout(layer['dropout_rate']))\n",
    "            print(f\"model.add(Dropout({layer['dropout_rate']}))\")\n",
    "\n",
    "        elif layer['model'] == 'Dense':\n",
    "            #{'model': 'Dense','units':64,'activation':relu'}, \n",
    "            #model.add(Dense(100, activation='relu'))\n",
    "            model.add(Dense(units=layer['units'], activation=layer['activation']))\n",
    "            print(f\"model.add(Dense(units={layer['units']}, activation={layer['activation']}))\")\n",
    "\n",
    "        elif layer['model'] == 'Flatten':\n",
    "            model.add(Flatten())\n",
    "            print(f'model.add(Flatten())')\n",
    "        else:\n",
    "            raise IOError ('invalid layer')\n",
    "        \n",
    "    #Create the model name\n",
    "    import datetime\n",
    "    now = datetime.datetime.now()\n",
    "    modelName = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "    try:\n",
    "        print(model.summary())\n",
    "    except:\n",
    "        pass\n",
    "    return model, modelName\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModelComplete(results_path):\n",
    "    from keras.utils import to_categorical\n",
    "    \n",
    "    #--- PARAMETERS --- --- --- ---- --- --- ---- ---- --- ----- --- --- ----\n",
    "    #notes from website:\n",
    "    #-- Common values are 50, 100, and 300. We will use 50 here, --\n",
    "    #-- but consider testing smaller or larger values. --\n",
    "    #-- We will use a two LSTM hidden layers with 100 memory cells each. --\n",
    "    #-- More memory cells and a deeper network may achieve better results. --\n",
    "    drseuss_text = 'data/combinedText.txt'\n",
    "    seed_length = 50\n",
    "    length = seed_length + 1\n",
    "    epochs = 2\n",
    "    batch_size = 128\n",
    "    #-- ---- ---- --- ---- ----- ---- ----- ---- ----- ----- ---- ---- ---- ----\n",
    "    \n",
    "    #-- load document --- --- --- --- --\n",
    "    drseuss_text = 'data/combinedText.txt'\n",
    "    tokens = load_doc(drseuss_text)\n",
    "\n",
    "    #-- Create sequencer and tokenizer -- --- --- --- --- --- --- --- \n",
    "    sequences, tokenizer = sequencesCreate(length, tokens)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    #-- Creating X, y -- --- --- --- --- --- --- -- --\n",
    "    df = pd.DataFrame(sequences)\n",
    "    print(f'sequences:\\n{df.head(5)}')\n",
    "    X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "    seq_length = X.shape[1]\n",
    "    input_shape = X.shape\n",
    "    #-- One hot encoding -- --- --- --- --- --- -\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "    print(f'seq_length: {seq_length}\\nshape of X: {X.shape}\\nshape of y: {y.shape}')\n",
    "    #-- -- ---- --- --- --- --- --- ---- --- --- --- --\n",
    "\n",
    "    #-- Model List --- --- --- --- --- --- --- --- --- -- ---- --- --- --- ---- -- --\n",
    "#     modelList = [{'model':'Embedding', 'input_dim':vocab_size, 'output_dim': 256, 'input_length': seq_length},\n",
    "#                  {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': 0, 'return_sequences': True}, \n",
    "#                  {'model': 'Dense','units':64,'activation':'relu'}, \n",
    "#                  {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': 0, 'return_sequences': True}, \n",
    "#                  {'model': 'Dense','units':64,'activation':'relu'}, \n",
    "#                  {'model':'Flatten'},\n",
    "#                  {'model': 'Dense','units':vocab_size,'activation':'softmax'},\n",
    "#                 ]\n",
    "    modelList = [{'model':'Embedding', 'input_dim':vocab_size, 'output_dim': 512, 'input_length': seq_length},\n",
    "                 {'model': 'LSTM', 'units':512, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': 0, 'return_sequences': True}, \n",
    "                 {'model': 'Dense','units':100,'activation':'relu'}, \n",
    "#                  {'model': 'LSTM', 'units':512, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': 0, 'return_sequences': True}, \n",
    "#                  {'model': 'Dense','units':100,'activation':'relu'}, \n",
    "                 {'model':'Flatten'},\n",
    "                 {'model': 'Dense','units':vocab_size,'activation':'softmax'},\n",
    "                ]\n",
    "\n",
    "    #-- --- ---- --- ---- --- --- ---- --- ---- --- ---- --- ---- --- --- --- --- ---\n",
    "    \n",
    "    print(f'drseuss_text: \\'{drseuss_text}\\'\\nseed_length: {seed_length}\\nepochs: {epochs}\\nbatch_size: {batch_size}'\n",
    "     f'\\nmodelList: {modelList}')\n",
    "    \n",
    "    #-- Create Model -- --- --- --- ---- --- -- ---- --- --- --- --- --- --- ---- --- ---\n",
    "    model, modelName = defineModel(vocab_size, seq_length, modelList, length, input_shape)\n",
    "    #-- save the tokenizer --- --- --- ---- --- --- ---- --\n",
    "    dump(tokenizer, open(results_path.rstrip('/').lstrip('/') + f'/token_'+modelName+'.pkl', 'wb'))\n",
    "    #-- Save history and final model --- -\n",
    "    writeFiles(modelName, modelList, seq_length, len(sequences), epochs, batch_size, results_path)\n",
    "    #-- Fit model -- ---- --- --- --- ---- --- --- ---- --- --- --- --- --- --- --- --- \n",
    "    history_callback = modelFit(model, modelName, X, y, seq_length, batch_size, epochs, results_path)\n",
    "    loss_history = history_callback.history\n",
    "    with open(results_path.rstrip('/').lstrip('/') + f'/loss_history_{modelName}.txt', 'w+') as f:\n",
    "        f.write(str(loss_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "#def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "def generate_seq(seq_length, seed_text, n_words, filepath = '', modelName = '', tokenizerName = '', ):\n",
    "    from keras.models import load_model\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    import re\n",
    "\n",
    "    if filepath :\n",
    "        highest_epoch = 0\n",
    "        for filename in os.listdir(filepath):\n",
    "            m = re.search('^wi_(\\d+)_', filename)\n",
    "            if m:\n",
    "                if int(m.group(1)) > highest_epoch:\n",
    "                    highest_epoch = int(m.group(1))\n",
    "                    modelName = filepath+'/'+filename\n",
    "            if re.search('token', filename):\n",
    "                tokenizerName = filepath+'/'+filename\n",
    "        \n",
    "    # load the model\n",
    "    model = load_model(modelName)\n",
    "\n",
    "    # load the tokenizer\n",
    "    tokenizer = load(open(tokenizerName, 'rb'))\n",
    "    \n",
    "    #Make 50 words long\n",
    "    seed_text = ' '.join(seed_text.split(' ')[0:seq_length])\n",
    "    \n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    \n",
    "    del model\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelList = [{'model':'Embedding', 'input_dim':2830, 'output_dim': 256, 'input_length': 50},\n",
    "#                  {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': 0, 'return_sequences': True}, \n",
    "#                  {'model': 'Dense','units':64,'activation':'relu'}, \n",
    "#                  {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': 0, 'return_sequences': True}, \n",
    "#                  {'model': 'Dense','units':64,'activation':'relu'}, \n",
    "#                  {'model':'Flatten'},\n",
    "#                  {'model': 'Dense','units':2830,'activation':'softmax'},\n",
    "#                 ]\n",
    "# history_callback = {'history':{'loss': [6.8130, 6.3438, 6.0809, 5.6680, 5.0674, 4.1888, 3.2263, 2.4416, 1.8358, 1.3483, 0.9936, 0.7174, 0.5278, 0.3948, 0.2838, 0.2132, 0.1515, 0.1078, 0.0862, 0.0653, 0.0591, 0.0499, 0.0395, 0.0275, 0.0271, 0.0293, 0.0370, 0.0441, 0.0782, 0.1003, 0.0644, 0.0407, 0.0296, 0.0202, 0.0133, 0.0067, 0.0048, 0.0053, 0.0050, 0.0076, 0.0120, 0.0162, 0.0466, 0.1344, 0.1101, 0.0600, 0.0288, 0.0118, 0.0063], \n",
    "#                     'acc':  [0.0366, 0.0477, 0.0514, 0.0527, 0.0647, 0.1239, 0.1239, 0.4201, 0.5374, 0.6495, 0.7304, 0.7957, 0.8472, 0.8845, 0.9168, 0.9394, 0.9593, 0.9714, 0.9791, 0.9854, 0.9866, 0.9900, 0.9918, 0.9948, 0.9946, 0.9945, 0.9913, 0.9897, 0.9782, 0.9701, 0.9821, 0.9881, 0.9925, 0.9952, 0.9975, 0.9991, 0.9997, 0.9996, 0.9996, 0.9987, 0.9984, 0.9962, 0.9854, 0.9570, 0.9661, 0.9805, 0.9917, 0.9974, 0.9993]}}\n",
    "# writeFiles('NULL', '2018-10-22_11-31', history_callback, modelList, 50, total_sequences = 16175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yertle', 'the', 'turtle', 'on', 'the', 'far', 'away', 'island', 'of', 'sala', 'ma', 'sond', 'yertle', 'the', 'turtle', 'was', 'king', 'of', 'the', 'pond.', 'a', 'nice', 'little', 'pond.', 'it', 'was', 'clean.', 'it', 'was', 'neat.', 'the', 'water', 'was', 'warm.', 'there', 'was', 'plenty', 'to', 'eat.', 'the', 'turtles', 'had', 'everything', 'turtles', 'might', 'need.', 'and', 'they', 'were', 'all', 'happy.', 'quite', 'happy', 'indeed.', 'they', 'were.', 'untill', 'yertle', 'the', 'king', 'of', 'them', 'all', 'decided', 'the', 'kingdom', 'he', 'ruled', 'was', 'too', 'small.', 'im', 'ruler', 'said', 'yertle', 'of', 'all', 'that', 'i', 'see.', 'but', 'i', 'dont', 'see', 'enough.', 'thats', 'the', 'trouble', 'with', 'me.', 'with', 'this', 'stone', 'for', 'a', 'throne', 'i', 'look', 'down', 'on']\n",
      "Total Tokens: 16226\n",
      "Unique Tokens: 2829\n",
      "Total Sequences: 16175\n",
      "sequences[0][0]: yertle\n",
      "sequences:\n",
      "    0    1    2     3     4     5     6     7     8     9   ...     41    42  \\\n",
      "0  162    1  161    12     1   237   425  2828     9   876  ...     47  1360   \n",
      "1    1  161   12     1   237   425  2828     9   876   502  ...   1360   214   \n",
      "2  161   12    1   237   425  2828     9   876   502  1362  ...    214   873   \n",
      "3   12    1  237   425  2828     9   876   502  1362   162  ...    873   641   \n",
      "4    1  237  425  2828     9   876   502  1362   162     1  ...    641     2   \n",
      "\n",
      "    43   44   45    46    47    48    49    50  \n",
      "0  214  873  641     2    15    78    16  1363  \n",
      "1  873  641    2    15    78    16  1363    95  \n",
      "2  641    2   15    78    16  1363    95   642  \n",
      "3    2   15   78    16  1363    95   642   877  \n",
      "4   15   78   16  1363    95   642   877    15  \n",
      "\n",
      "[5 rows x 51 columns]\n",
      "seq_length: 50\n",
      "shape of X: (16175, 50)\n",
      "shape of y: (16175, 2830)\n",
      "drseuss_text: 'data/combinedText.txt'\n",
      "seed_length: 50\n",
      "epochs: 50\n",
      "batch_size: 128\n",
      "modelList: [{'model': 'Embedding', 'input_dim': 2830, 'output_dim': 512, 'input_length': 50}, {'model': 'LSTM', 'units': 512, 'use_bias': True, 'dropout': 0.2, 'recurrent_dropout': 0, 'return_sequences': True}, {'model': 'Dense', 'units': 100, 'activation': 'relu'}, {'model': 'LSTM', 'units': 512, 'use_bias': True, 'dropout': 0.2, 'recurrent_dropout': 0, 'return_sequences': True}, {'model': 'Dense', 'units': 100, 'activation': 'relu'}, {'model': 'Flatten'}, {'model': 'Dense', 'units': 2830, 'activation': 'softmax'}]\n",
      "model.add(Embedding(input_dim= 2830, output_dim=512, input_length=50))\n",
      "model.add(LSTM(units=512, use_bias=True, dropout=0.2, recurrent_dropout=0 ))\n",
      "model.add(Dense(units=100, activation=relu))\n",
      "model.add(LSTM(units=512, use_bias=True, dropout=0.2, recurrent_dropout=0 ))\n",
      "model.add(Dense(units=100, activation=relu))\n",
      "model.add(Flatten())\n",
      "model.add(Dense(units=2830, activation=softmax))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 512)           1448960   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50, 100)           51300     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 512)           1255424   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50, 100)           51300     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2830)              14152830  \n",
      "=================================================================\n",
      "Total params: 19,059,014\n",
      "Trainable params: 19,059,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "  128/16175 [..............................] - ETA: 14:44 - loss: 7.9478 - acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-51c1a4b958c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrainModelComplete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-1f952b296c8c>\u001b[0m in \u001b[0;36mtrainModelComplete\u001b[0;34m(results_path)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'token_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodelName\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m#-- Fit model -- ---- --- --- --- ---- --- --- ---- --- --- --- --- --- --- --- ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mhistory_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelFit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;31m#-- Save history and final model --- -\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mwriteFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b4547fb07441>\u001b[0m in \u001b[0;36mmodelFit\u001b[0;34m(model, modelName, X, y, seq_length, batch_size, epochs, results_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mhistory_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    trainModelComplete('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainModelComplete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_create(filepath = '.'):\n",
    "    import os, ast, json, re, seed\n",
    "    datetime = {}\n",
    "    #-- Determine JSON file name -- \n",
    "    jsonFile = f'{filepath}/Alldata.json'; i = '0'\n",
    "    while os.path.isfile(jsonFile):\n",
    "        i = str(int(i)+1)\n",
    "        jsonFile = f\"{filepath}/Alldata{i}.json\"\n",
    "        \n",
    "    for filename in os.listdir(filepath):\n",
    "        #wi_01_6.7077__2018-10-22_09-29.hdf5\n",
    "        m = re.search('wi_(..)_(......)__*(....-..-..)_(..-..).hdf5', filename)\n",
    "        if m:\n",
    "            epoch, loss, date, time = m.group(1), m.group(2), m.group(3), m.group(4)\n",
    "            if date+'_'+time not in datetime.keys():\n",
    "                #print(f\"{date+'_'+time} not in KEYS: \\n{datetime.keys()}\")\n",
    "                tokenizer = filepath+f'/token_{date}_{time}.pkl'\n",
    "                try:\n",
    "                    with open(filepath.rstrip('/').lstrip('/')+'/info_' + date+'_'+time + '.txt') as f:\n",
    "                        text = f.read()\n",
    "                    modelList = text.split(']')[0] + ']'\n",
    "                    modelHistory = '{' + ']'.join(text.split(']')[1:]).split('{')[1]\n",
    "                    #print(f\"NEW DATA: {date+'_'+time}\")\n",
    "                    modelHistory = ast.literal_eval(modelHistory)\n",
    "                    modelList = ast.literal_eval(modelList)\n",
    "                    epochs = modelHistory['epochs']\n",
    "                    if os.path.isfile(f\"{date+'_'+time}_loss_history.txt\"):\n",
    "                        with open(f\"{filepath.rstrip('/').lstrip('/')}/{date+'_'+time}_loss_history.txt\") as f:\n",
    "                            model_history = f.read()\n",
    "                        model_history = ast.literal_eval(model_history)\n",
    "                        modelHistory['model_history'] = model_history\n",
    "                except:\n",
    "                    modelList = []\n",
    "                    modelHistory = {}\n",
    "                datetime[date+'_'+time] = {'model_list': modelList,\n",
    "                                           'model_history': modelHistory,\n",
    "                                           'sequence_list': ['no_model_data']*(epochs+1)}\n",
    "                try:\n",
    "                    seq_length = modelHistory['seq_length']\n",
    "                except:\n",
    "                    seq_length = 50\n",
    "                #print(f'{epoch}: {datetime}')\n",
    "            #seq_length, seed_text, n_words, filepath = '', modelName = '', tokenizerName = '', )\n",
    "            datetime[date+'_'+time]['sequence_list'][int(epoch)] = generate_seq(seq_length, seed.seed_text, 50, filepath, filename, tokenizer) \n",
    "            print('\\n',filename, \": \",datetime[date+'_'+time]['sequence_list'][int(epoch)])\n",
    "            #-- Write JSON file -- --- ----\n",
    "            with open(jsonFile, 'w+') as fp:\n",
    "                json.dump(datetime, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wi_76_0.0010__51_LSTM_256_True_Dense_256_relu_Dropout_0.2__LSTM_128_True_Dense_128_relu_Dropout_0.2__LSTM_64_False_Dense_64_relu_Flatten___Dense_2830_softmax.hdf\n",
    "def jsonify_the_old_style_file(filepath = '.'):\n",
    "    import seed, re, os, json\n",
    "    jsonFile = filepath + '/Alldata.json'; i = '0'\n",
    "    #-- Determine JSON file name -- \n",
    "    while os.path.isfile(jsonFile):\n",
    "        i = str(int(i)+1)\n",
    "        jsonFile = f\"{filepath}/Alldata{i}.json\"\n",
    "    tokenizer = filepath + '/toke_51_LSTM_256_True_Dense_256_relu_Dropout_0.2__LSTM_128_True_Dense_128_relu_Dropout_0.2__LSTM_64_False_Dense_64_relu_Flatten___Dense_2830_softmax.pkl'\n",
    "    jsondict = {'sequences': ['no_data']*112, 'model':None, 'loss': ['no_data']*112}\n",
    "    for filename in os.listdir(filepath):\n",
    "        m = re.search('wi_(..)_(......)__(.*).hdf5', filename)\n",
    "        if m and re.search('51_LSTM_256_True_Dense_256_relu_Dropout_0.2__LSTM_128_True_Dense_128_relu_Dropout_0.2__LSTM_64_False_Dense_64_relu_Flatten___Dense_2830_softmax', filename):\n",
    "            epoch, loss, modellist = m.group(1), m.group(2), m.group(3)\n",
    "            jsondict['model'] = modellist\n",
    "            jsondict['loss'][int(epoch)] = float(loss)\n",
    "            #seq_length, seed_text, n_words, filepath = '', modelName = '', tokenizerName = '', )\n",
    "            jsondict['sequences'][int(epoch)] = generate_seq(50, seed.seed_text, 50, os.path.join(filepath,filename), tokenizer, 50, seed.seed_text, 50)\n",
    "            print(epoch, ': ', jsondict['sequences'][int(epoch)])\n",
    "            #-- Write JSON file -- --- ----\n",
    "            with open(jsonFile, 'w+') as fp:\n",
    "                json.dump(jsondict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.4416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.8358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.3483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.9936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.7174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.5278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.3948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.2838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.2132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.1515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.1078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.1003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.0053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.1344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.0288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.0118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss\n",
       "0   6.8130\n",
       "1   6.3438\n",
       "2   6.0809\n",
       "3   5.6680\n",
       "4   5.0674\n",
       "5   4.1888\n",
       "6   3.2263\n",
       "7   2.4416\n",
       "8   1.8358\n",
       "9   1.3483\n",
       "10  0.9936\n",
       "11  0.7174\n",
       "12  0.5278\n",
       "13  0.3948\n",
       "14  0.2838\n",
       "15  0.2132\n",
       "16  0.1515\n",
       "17  0.1078\n",
       "18  0.0862\n",
       "19  0.0653\n",
       "20  0.0591\n",
       "21  0.0499\n",
       "22  0.0395\n",
       "23  0.0275\n",
       "24  0.0271\n",
       "25  0.0293\n",
       "26  0.0370\n",
       "27  0.0441\n",
       "28  0.0782\n",
       "29  0.1003\n",
       "30  0.0644\n",
       "31  0.0407\n",
       "32  0.0296\n",
       "33  0.0202\n",
       "34  0.0133\n",
       "35  0.0067\n",
       "36  0.0048\n",
       "37  0.0053\n",
       "38  0.0050\n",
       "39  0.0076\n",
       "40  0.0120\n",
       "41  0.0162\n",
       "42  0.0466\n",
       "43  0.1344\n",
       "44  0.1101\n",
       "45  0.0600\n",
       "46  0.0288\n",
       "47  0.0118\n",
       "48  0.0063"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_json('Alldata3.json')\n",
    "df['loss'] = df['loss'].replace('no_data', 'nan') \n",
    "df=df.truncate(before=1, after=48)\n",
    "df.head()\n",
    "\n",
    "df2=pd.read_json('Alldata.json')\n",
    "data2Dict = df2.to_dict()\n",
    "df2=pd.DataFrame()\n",
    "df2['loss'] = data2Dict['2018-10-22_11-31']['model_history']['loss']\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np; np.random.seed(1)\n",
    "\n",
    "x = df.index\n",
    "y = df['loss']\n",
    "names = df['sequences']\n",
    "#c = np.random.randint(1,5,size=15)\n",
    "\n",
    "#norm = plt.Normalize(1,4)\n",
    "cmap = plt.cm.RdYlGn\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "#sc = plt.scatter(x,y,c=c, s=100, cmap=cmap, norm=norm)\n",
    "sc = plt.plot(x,y, 'r', df2.index, df2['loss'], 'b')#, cmap=cmap)\n",
    "\n",
    "annot = ax.annotate(\"\", xy=(0,0), xytext=(20,20),textcoords=\"offset points\",\n",
    "                    bbox=dict(boxstyle=\"round\", fc=\"w\"),\n",
    "                    arrowprops=dict(arrowstyle=\"->\"))\n",
    "annot.set_visible(False)\n",
    "\n",
    "def update_annot(ind):\n",
    "\n",
    "    pos = sc.get_offsets()[ind[\"ind\"][0]]\n",
    "    annot.xy = pos\n",
    "    text = \"{}, {}\".format(\" \".join(list(map(str,ind[\"ind\"]))), \n",
    "                           \" \".join([names[n] for n in ind[\"ind\"]]))\n",
    "    annot.set_text(text)\n",
    "    annot.get_bbox_patch().set_facecolor(cmap(norm(c[ind[\"ind\"][0]])))\n",
    "    annot.get_bbox_patch().set_alpha(0.4)\n",
    "\n",
    "\n",
    "def hover(event):\n",
    "    vis = annot.get_visible()\n",
    "    if event.inaxes == ax:\n",
    "        cont, ind = sc.contains(event)\n",
    "        if cont:\n",
    "            update_annot(ind)\n",
    "            annot.set_visible(True)\n",
    "            fig.canvas.draw_idle()\n",
    "        else:\n",
    "            if vis:\n",
    "                annot.set_visible(False)\n",
    "                fig.canvas.draw_idle()\n",
    "\n",
    "fig.canvas.mpl_connect(\"motion_notify_event\", hover)\n",
    "plt.ylabel('Model Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('Model Loss per Epoch')\n",
    "plt.show()\n",
    "plt.savefig('ModelLoss.PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
