{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, sys, os, pandas as pd\n",
    "from random import randint\n",
    "from pickle import dump, load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    tokens = text.split()\n",
    "    print(tokens[:100])\n",
    "    print('Total Tokens: %d' % len(tokens))\n",
    "    print('Unique Tokens: %d' % len(set(tokens)))\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize into sequences of tokens\n",
    "#the plus one is because the last val in the list will be the expected prediction. \n",
    "#Its our Y-train\n",
    "def sequencesCreate(length, tokens):\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    sequences = list()\n",
    "    for i in range(length, len(tokens)):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[i-length:i]\n",
    "        # convert into a line\n",
    "        #line = ' '.join(seq)\n",
    "        # store\n",
    "        sequences.append(seq)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    print(f'sequences[0][0]: {sequences[0][0]}')\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    # integer encode sequences of words\n",
    "    #sequences = [str(i) for i in sequences]\n",
    "    # print(f'tokenizer: {tokenizer}')\n",
    "    tokenizer.fit_on_texts(sequences)\n",
    "    # print(f'tokenizer: {tokenizer}')\n",
    "    sequences = tokenizer.texts_to_sequences(sequences)\n",
    "    # print(f'sequences: {sequences}')\n",
    "    \n",
    "    return sequences, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelFit(model, modelName, X, y, seq_length, batch_size, epochs, results_path):\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # define the checkpoint\n",
    "    filepath=f\"{results_path.rstrip('/').lstrip('/')}/wi_{{epoch:02d}}_{{loss:.4f}}_{modelName}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    # fit model\n",
    "    history_callback = model.fit(X, y, batch_size=batch_size, epochs=epochs, callbacks=callbacks_list)\n",
    "    return history_callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- --- ---- --- ---- --- ---- ---- --- ----- ---- ---\n",
    "# -- Write Files ---- ---- ---- --- ---- --- --- --- -- \n",
    "#--- --- ---- --- ---- --- ---- ---- --- ----- ---- ---\n",
    "def writeFiles(modelName, modelList, seq_length, total_sequences, epochs, batch_size, results_path):\n",
    "    model_info = {} #history_callback.history\n",
    "    model_info['seq_length'] = seq_length\n",
    "    model_info['total_sequences'] = total_sequences\n",
    "    model_info['batch_size'] = batch_size\n",
    "    model_info['epochs'] = epochs\n",
    "    \n",
    "    # save losses\n",
    "    rFile = results_path.rstrip('/').lstrip('/') + '/info_' + modelName + '.txt'\n",
    "    print(f'Info File: {rFile}')\n",
    "    with open(rFile,'w+') as f:\n",
    "        f.write(str(modelList))\n",
    "        f.write('\\n')\n",
    "        f.write(str(model_info))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "def defineModel(vocab_size, seq_length, modelList, length, input_shape):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import Dropout\n",
    "    from keras.layers import LSTM\n",
    "    from keras.utils import np_utils\n",
    "    from keras.layers import Embedding, Flatten\n",
    "    model = Sequential()\n",
    "    #-- EMBEDDED LAYER --- --- --- ---- --\n",
    "    #input_dim: size of the vocabulary in the text data.\n",
    "    #output_dim: size of the vector space where words will be embedded. or size of the output vectors from this layer try 32 or 100 or larger\n",
    "    #input_length: length of input seq's. ex: if input documents are comprised of 1000 words, it would be 1000.\n",
    "#     modelList = [{'model':'Embedding', 'input_dim':vocab_size, 'output_dim': 100, 'input_length': seq_length},\n",
    "#                  {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': .2}, \n",
    "#                  {'model': 'Dense','units':64,'activation':'relu'}, \n",
    "#                  {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': .2}, \n",
    "#                  {'model': 'Dense','units':64,'activation':'relu'}, \n",
    "#                  {'model':'Flatten'},\n",
    "#                  {'model': 'Dense','units':vocab_size,'activation':'softmax'},\n",
    "#                 ]\n",
    "    for i,layer in enumerate(modelList):\n",
    "        if layer['model'] == 'Embedding': \n",
    "            model.add(Embedding(input_dim=layer['input_dim'], output_dim=layer['output_dim'], \n",
    "                                input_length=layer['input_length']))\n",
    "\n",
    "            print(f\"model.add(Embedding(input_dim= {layer['input_dim']}, output_dim={ layer['output_dim'] }, input_length={ layer['input_length'] }))\")\n",
    "        elif layer['model'] == 'LSTM':\n",
    "            #model.add(LSTM(100, return_sequences=True))\n",
    "            #model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, input_dim=1))\n",
    "            model.add(LSTM(units=layer['units'], use_bias=layer['use_bias'], \n",
    "                           dropout=layer['dropout'], recurrent_dropout=layer['recurrent_dropout'], \n",
    "                           return_sequences = layer['return_sequences']))\n",
    "            print(f\"model.add(LSTM(units={layer['units']}, use_bias={layer['use_bias']}, dropout={layer['dropout']}, recurrent_dropout={layer['recurrent_dropout']} ))\")\n",
    "\n",
    "        elif layer['model'] == 'Dropout':\n",
    "            #model.add(Dropout(0.2))\n",
    "            model.add(Dropout(layer['dropout_rate']))\n",
    "            print(f\"model.add(Dropout({layer['dropout_rate']}))\")\n",
    "\n",
    "        elif layer['model'] == 'Dense':\n",
    "            #{'model': 'Dense','units':64,'activation':relu'}, \n",
    "            #model.add(Dense(100, activation='relu'))\n",
    "            model.add(Dense(units=layer['units'], activation=layer['activation']))\n",
    "            print(f\"model.add(Dense(units={layer['units']}, activation={layer['activation']}))\")\n",
    "\n",
    "        elif layer['model'] == 'Flatten':\n",
    "            model.add(Flatten())\n",
    "            print(f'model.add(Flatten())')\n",
    "        else:\n",
    "            raise IOError ('invalid layer')\n",
    "        \n",
    "    #Create the model name\n",
    "    import datetime\n",
    "    now = datetime.datetime.now()\n",
    "    modelName = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "    try:\n",
    "        print(model.summary())\n",
    "    except:\n",
    "        pass\n",
    "    return model, modelName\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModelComplete(results_path):\n",
    "    from keras.utils import to_categorical\n",
    "    \n",
    "    #--- PARAMETERS --- --- --- ---- --- --- ---- ---- --- ----- --- --- ----\n",
    "    #notes from website:\n",
    "    #-- Common values are 50, 100, and 300. We will use 50 here, --\n",
    "    #-- but consider testing smaller or larger values. --\n",
    "    #-- We will use a two LSTM hidden layers with 100 memory cells each. --\n",
    "    #-- More memory cells and a deeper network may achieve better results. --\n",
    "    drseuss_text = 'data/combinedText.txt'\n",
    "    seed_length = 50\n",
    "    length = seed_length + 1\n",
    "    epochs = 3\n",
    "    batch_size = 128\n",
    "    #-- ---- ---- --- ---- ----- ---- ----- ---- ----- ----- ---- ---- ---- ----\n",
    "    \n",
    "    #-- load document --- --- --- --- --\n",
    "    drseuss_text = 'data/combinedText.txt'\n",
    "    tokens = load_doc(drseuss_text)\n",
    "\n",
    "    #-- Create sequencer and tokenizer -- --- --- --- --- --- --- --- \n",
    "    sequences, tokenizer = sequencesCreate(length, tokens)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    #-- Creating X, y -- --- --- --- --- --- --- -- --\n",
    "    df = pd.DataFrame(sequences)\n",
    "    print(f'sequences:\\n{df.head(5)}')\n",
    "    X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "    seq_length = X.shape[1]\n",
    "    input_shape = X.shape\n",
    "    #-- One hot encoding -- --- --- --- --- --- -\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "    print(f'seq_length: {seq_length}\\nshape of X: {X.shape}\\nshape of y: {y.shape}')\n",
    "    #-- -- ---- --- --- --- --- --- ---- --- --- --- --\n",
    "\n",
    "    #-- Model List --- --- --- --- --- --- --- --- --- -- ---- --- --- --- ---- -- --\n",
    "#     modelList = [{'model':'Embedding', 'input_dim':vocab_size, 'output_dim': 256, 'input_length': seq_length},\n",
    "#                  {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': 0, 'return_sequences': True}, \n",
    "#                  {'model': 'Dense','units':64,'activation':'relu'}, \n",
    "#                  {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': 0, 'return_sequences': True}, \n",
    "#                  {'model': 'Dense','units':64,'activation':'relu'}, \n",
    "#                  {'model':'Flatten'},\n",
    "#                  {'model': 'Dense','units':vocab_size,'activation':'softmax'},\n",
    "#                 ]\n",
    "    modelList = [{'model':'Embedding', 'input_dim':vocab_size, 'output_dim': 512, 'input_length': seq_length},\n",
    "                 {'model': 'LSTM', 'units':512, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': 0, 'return_sequences': True}, \n",
    "                 {'model': 'Dense','units':100,'activation':'relu'}, \n",
    "                 {'model': 'LSTM', 'units':512, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': 0, 'return_sequences': True}, \n",
    "                 {'model': 'Dense','units':100,'activation':'relu'}, \n",
    "                 {'model':'Flatten'},\n",
    "                 {'model': 'Dense','units':vocab_size,'activation':'softmax'},\n",
    "                ]\n",
    "\n",
    "    #-- --- ---- --- ---- --- --- ---- --- ---- --- ---- --- ---- --- --- --- --- ---\n",
    "    \n",
    "    print(f'drseuss_text: \\'{drseuss_text}\\'\\nseed_length: {seed_length}\\nepochs: {epochs}\\nbatch_size: {batch_size}'\n",
    "     f'\\nmodelList: {modelList}')\n",
    "    \n",
    "    #-- Create Model -- --- --- --- ---- --- -- ---- --- --- --- --- --- --- ---- --- ---\n",
    "    model, modelName = defineModel(vocab_size, seq_length, modelList, length, input_shape)\n",
    "    #-- save the tokenizer --- --- --- ---- --- --- ---- --\n",
    "    dump(tokenizer, open(results_path.rstrip('/').lstrip('/') + f'/token_'+modelName+'.pkl', 'wb'))\n",
    "    #-- Save history and final model --- -\n",
    "    writeFiles(modelName, modelList, seq_length, len(sequences), epochs, batch_size, results_path)\n",
    "    #-- Fit model -- ---- --- --- --- ---- --- --- ---- --- --- --- --- --- --- --- --- \n",
    "    history_callback = modelFit(model, modelName, X, y, seq_length, batch_size, epochs, results_path)\n",
    "    loss_history = history_callback.history\n",
    "    with open(results_path.rstrip('/').lstrip('/') + f'/{modelName}_loss_history.txt') as f:\n",
    "        f.write(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "#def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "def generate_seq(modelName, tokenizerName, seq_length, seed_text, n_words):\n",
    "    from keras.models import load_model\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    # load the model\n",
    "    model = load_model(modelName)\n",
    "\n",
    "    # load the tokenizer\n",
    "    tokenizer = load(open(tokenizerName, 'rb'))\n",
    "    \n",
    "    #Make 50 words long\n",
    "    seed_text = ' '.join(seed_text.split(' ')[0:seq_length])\n",
    "    \n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    \n",
    "    del model\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelList = [{'model':'Embedding', 'input_dim':2830, 'output_dim': 256, 'input_length': 50},\n",
    "#                  {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': 0, 'return_sequences': True}, \n",
    "#                  {'model': 'Dense','units':64,'activation':'relu'}, \n",
    "#                  {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': 0, 'return_sequences': True}, \n",
    "#                  {'model': 'Dense','units':64,'activation':'relu'}, \n",
    "#                  {'model':'Flatten'},\n",
    "#                  {'model': 'Dense','units':2830,'activation':'softmax'},\n",
    "#                 ]\n",
    "# history_callback = {'history':{'loss': [6.8130, 6.3438, 6.0809, 5.6680, 5.0674, 4.1888, 3.2263, 2.4416, 1.8358, 1.3483, 0.9936, 0.7174, 0.5278, 0.3948, 0.2838, 0.2132, 0.1515, 0.1078, 0.0862, 0.0653, 0.0591, 0.0499, 0.0395, 0.0275, 0.0271, 0.0293, 0.0370, 0.0441, 0.0782, 0.1003, 0.0644, 0.0407, 0.0296, 0.0202, 0.0133, 0.0067, 0.0048, 0.0053, 0.0050, 0.0076, 0.0120, 0.0162, 0.0466, 0.1344, 0.1101, 0.0600, 0.0288, 0.0118, 0.0063], \n",
    "#                     'acc':  [0.0366, 0.0477, 0.0514, 0.0527, 0.0647, 0.1239, 0.1239, 0.4201, 0.5374, 0.6495, 0.7304, 0.7957, 0.8472, 0.8845, 0.9168, 0.9394, 0.9593, 0.9714, 0.9791, 0.9854, 0.9866, 0.9900, 0.9918, 0.9948, 0.9946, 0.9945, 0.9913, 0.9897, 0.9782, 0.9701, 0.9821, 0.9881, 0.9925, 0.9952, 0.9975, 0.9991, 0.9997, 0.9996, 0.9996, 0.9987, 0.9984, 0.9962, 0.9854, 0.9570, 0.9661, 0.9805, 0.9917, 0.9974, 0.9993]}}\n",
    "# writeFiles('NULL', '2018-10-22_11-31', history_callback, modelList, 50, total_sequences = 16175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yertle', 'the', 'turtle', 'on', 'the', 'far', 'away', 'island', 'of', 'sala', 'ma', 'sond', 'yertle', 'the', 'turtle', 'was', 'king', 'of', 'the', 'pond.', 'a', 'nice', 'little', 'pond.', 'it', 'was', 'clean.', 'it', 'was', 'neat.', 'the', 'water', 'was', 'warm.', 'there', 'was', 'plenty', 'to', 'eat.', 'the', 'turtles', 'had', 'everything', 'turtles', 'might', 'need.', 'and', 'they', 'were', 'all', 'happy.', 'quite', 'happy', 'indeed.', 'they', 'were.', 'untill', 'yertle', 'the', 'king', 'of', 'them', 'all', 'decided', 'the', 'kingdom', 'he', 'ruled', 'was', 'too', 'small.', 'im', 'ruler', 'said', 'yertle', 'of', 'all', 'that', 'i', 'see.', 'but', 'i', 'dont', 'see', 'enough.', 'thats', 'the', 'trouble', 'with', 'me.', 'with', 'this', 'stone', 'for', 'a', 'throne', 'i', 'look', 'down', 'on']\n",
      "Total Tokens: 16226\n",
      "Unique Tokens: 2829\n",
      "Total Sequences: 16175\n",
      "sequences[0][0]: yertle\n",
      "sequences:\n",
      "    0    1    2     3     4     5     6     7     8     9   ...     41    42  \\\n",
      "0  162    1  161    12     1   237   425  2828     9   876  ...     47  1360   \n",
      "1    1  161   12     1   237   425  2828     9   876   502  ...   1360   214   \n",
      "2  161   12    1   237   425  2828     9   876   502  1362  ...    214   873   \n",
      "3   12    1  237   425  2828     9   876   502  1362   162  ...    873   641   \n",
      "4    1  237  425  2828     9   876   502  1362   162     1  ...    641     2   \n",
      "\n",
      "    43   44   45    46    47    48    49    50  \n",
      "0  214  873  641     2    15    78    16  1363  \n",
      "1  873  641    2    15    78    16  1363    95  \n",
      "2  641    2   15    78    16  1363    95   642  \n",
      "3    2   15   78    16  1363    95   642   877  \n",
      "4   15   78   16  1363    95   642   877    15  \n",
      "\n",
      "[5 rows x 51 columns]\n",
      "seq_length: 50\n",
      "shape of X: (16175, 50)\n",
      "shape of y: (16175, 2830)\n",
      "drseuss_text: 'data/combinedText.txt'\n",
      "seed_length: 50\n",
      "epochs: 50\n",
      "batch_size: 128\n",
      "modelList: [{'model': 'Embedding', 'input_dim': 2830, 'output_dim': 512, 'input_length': 50}, {'model': 'LSTM', 'units': 512, 'use_bias': True, 'dropout': 0.2, 'recurrent_dropout': 0, 'return_sequences': True}, {'model': 'Dense', 'units': 100, 'activation': 'relu'}, {'model': 'LSTM', 'units': 512, 'use_bias': True, 'dropout': 0.2, 'recurrent_dropout': 0, 'return_sequences': True}, {'model': 'Dense', 'units': 100, 'activation': 'relu'}, {'model': 'Flatten'}, {'model': 'Dense', 'units': 2830, 'activation': 'softmax'}]\n",
      "model.add(Embedding(input_dim= 2830, output_dim=512, input_length=50))\n",
      "model.add(LSTM(units=512, use_bias=True, dropout=0.2, recurrent_dropout=0 ))\n",
      "model.add(Dense(units=100, activation=relu))\n",
      "model.add(LSTM(units=512, use_bias=True, dropout=0.2, recurrent_dropout=0 ))\n",
      "model.add(Dense(units=100, activation=relu))\n",
      "model.add(Flatten())\n",
      "model.add(Dense(units=2830, activation=softmax))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 512)           1448960   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50, 100)           51300     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 512)           1255424   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50, 100)           51300     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2830)              14152830  \n",
      "=================================================================\n",
      "Total params: 19,059,014\n",
      "Trainable params: 19,059,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "  128/16175 [..............................] - ETA: 14:44 - loss: 7.9478 - acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-51c1a4b958c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrainModelComplete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-1f952b296c8c>\u001b[0m in \u001b[0;36mtrainModelComplete\u001b[0;34m(results_path)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'token_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodelName\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m#-- Fit model -- ---- --- --- --- ---- --- --- ---- --- --- --- --- --- --- --- ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mhistory_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelFit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;31m#-- Save history and final model --- -\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mwriteFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b4547fb07441>\u001b[0m in \u001b[0;36mmodelFit\u001b[0;34m(model, modelName, X, y, seq_length, batch_size, epochs, results_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mhistory_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    trainModelComplete('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainModelComplete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_create(filepath = '.'):\n",
    "    import os, ast, json, re, seed\n",
    "    datetime = {}\n",
    "    #-- Determine JSON file name -- \n",
    "    jsonFile = 'Alldata.json'; i = '0'\n",
    "    while os.path.isfile(jsonFile):\n",
    "        i = str(int(i)+1)\n",
    "        jsonFile = f\"Alldata{i}.json\"\n",
    "        \n",
    "    for filename in os.listdir(filepath):\n",
    "        #wi_01_6.7077__2018-10-22_09-29.hdf5\n",
    "        m = re.search('wi_(..)_(......)__*(....-..-..)_(..-..).hdf5', filename)\n",
    "        if m:\n",
    "            epoch, loss, date, time = m.group(1), m.group(2), m.group(3), m.group(4)\n",
    "            if date+'_'+time not in datetime.keys():\n",
    "                #print(f\"{date+'_'+time} not in KEYS: \\n{datetime.keys()}\")\n",
    "                tokenizer = filepath+f'/token_{date}_{time}.pkl'\n",
    "                try:\n",
    "                    with open(filepath+'/info_' + date+'_'+time + '.txt') as f:\n",
    "                        text = f.read()\n",
    "                    modelList = text.split(']')[0] + ']'\n",
    "                    modelHistory = '{' + ']'.join(text.split(']')[1:]).split('{')[1]\n",
    "                    print(f\"NEW DATA: {date+'_'+time}\")\n",
    "                    modelHistory = ast.literal_eval(modelHistory)\n",
    "                    modelList = ast.literal_eval(modelList)\n",
    "                    epochs = modelHistory['epochs']\n",
    "                    if os.path.isfile(f\"{date+'_'+time}_loss_history.txt\"):\n",
    "                        with open(f\"{date+'_'+time}_loss_history.txt\") as f:\n",
    "                            model_history = f.read()\n",
    "                        model_history = ast.literal_eval(model_history)\n",
    "                        modelHistory['model_history'] = model_history\n",
    "                except:\n",
    "                    modelList = []\n",
    "                    modelHistory = {}\n",
    "                datetime[date+'_'+time] = {'model_list': modelList,\n",
    "                                           'model_history': modelHistory,\n",
    "                                           'sequence_list': ['no_model_data']*(epochs+1)}\n",
    "                print(datetime)\n",
    "            datetime[date+'_'+time]['sequence_list'][int(epoch)] = generate_seq(os.path.join(filepath,filename), tokenizer, 50, seed.seed_text, 50)\n",
    "            print('\\n',filename, \": \",datetime[date+'_'+time]['sequence_list'][int(epoch)])\n",
    "            #-- Write JSON file -- --- ----\n",
    "            with open(jsonFile, 'w+') as fp:\n",
    "                json.dump(datetime, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wi_76_0.0010__51_LSTM_256_True_Dense_256_relu_Dropout_0.2__LSTM_128_True_Dense_128_relu_Dropout_0.2__LSTM_64_False_Dense_64_relu_Flatten___Dense_2830_softmax.hdf\n",
    "def jsonify_the_old_style_file(filepath = '.'):\n",
    "    import seed, re, os, json\n",
    "    jsonFile = 'Alldata.json'; i = '0'\n",
    "    #-- Determine JSON file name -- \n",
    "    while os.path.isfile(jsonFile):\n",
    "        i = str(int(i)+1)\n",
    "        jsonFile = f\"Alldata{i}.json\"\n",
    "    tokenizer = 'toke_51_LSTM_256_True_Dense_256_relu_Dropout_0.2__LSTM_128_True_Dense_128_relu_Dropout_0.2__LSTM_64_False_Dense_64_relu_Flatten___Dense_2830_softmax.pkl'\n",
    "    jsondict = {'sequences': ['no_data']*112, 'model':None, 'loss': ['no_data']*112}\n",
    "    for filename in os.listdir(filepath):\n",
    "        m = re.search('wi_(..)_(......)__(.*).hdf5', filename)\n",
    "        if m and re.search('51_LSTM_256_True_Dense_256_relu_Dropout_0.2__LSTM_128_True_Dense_128_relu_Dropout_0.2__LSTM_64_False_Dense_64_relu_Flatten___Dense_2830_softmax', filename):\n",
    "            epoch, loss, modellist = m.group(1), m.group(2), m.group(3)\n",
    "            jsondict['model'] = modellist\n",
    "            jsondict['loss'][int(epoch)] = float(loss)\n",
    "            jsondict['sequences'][int(epoch)] = generate_seq(os.path.join(filepath,filename), tokenizer, 50, seed.seed_text, 50)\n",
    "            print(epochs, ': ', jsondict['sequences'][int(epoch)])\n",
    "            #-- Write JSON file -- --- ----\n",
    "            with open(jsonFile, 'w+') as fp:\n",
    "                json.dump(datetime, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
