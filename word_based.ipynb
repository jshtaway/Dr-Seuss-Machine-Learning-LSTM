{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 178,
=======
   "execution_count": 118,
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drseuss_text: 'data/combinedText.txt'\n",
      "seed_length: 50\n",
<<<<<<< HEAD
      "epochs: 500\n",
      "batch_size: 128\n",
      "modelName: m_51_LSTM_256_True_Dense_256_relu_Dropout_0.2__LSTM_128_True_Dense_128_relu_Dropout_0.2__LSTM_64_False_Dense_64_relu_Flatten___Dense_2830_softmax.h5\n",
      "tokenizerName: toke_51_LSTM_256_True_Dense_256_relu_Dropout_0.2__LSTM_128_True_Dense_128_relu_Dropout_0.2__LSTM_64_False_Dense_64_relu_Flatten___Dense_2830_softmax.pkl\n",
      "modelList: [('LSTM', 256, 'True'), ('Dense', 256, 'relu'), ('Dropout', 0.2, ''), ('LSTM', 128, 'True'), ('Dense', 128, 'relu'), ('Dropout', 0.2, ''), ('LSTM', 64, 'False'), ('Dense', 64, 'relu'), ('Flatten', '', ''), ('Dense', 2830, 'softmax')]\n"
=======
      "epochs: 100\n",
      "batch_size: 128\n",
      "modelName: m_51_LSTM_100_True_Dropout_0.2__LSTM_100_False_Dense_100_relu_Dense_2830_softmax.h5\n",
      "tokenizerName: toke_51_LSTM_100_True_Dropout_0.2__LSTM_100_False_Dense_100_relu_Dense_2830_softmax.pkl\n",
      "modelList: [('LSTM', 100, 'True'), ('Dropout', 0.2, ''), ('LSTM', 100, 'False'), ('Dense', 100, 'relu'), ('Dense', 2830, 'softmax')]\n"
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from pickle import dump\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import sys\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
<<<<<<< HEAD
    "from keras.layers import Embedding, Flatten\n",
    "\n",
    "#notes from website:\n",
    "#-- Common values are 50, 100, and 300. We will use 50 here, --\n",
    "#-- but consider testing smaller or larger values. --\n",
    "#-- We will use a two LSTM hidden layers with 100 memory cells each. --\n",
    "#-- More memory cells and a deeper network may achieve better results. --\n",
=======
    "from keras.layers import Embedding\n",
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
    "\n",
    "#parameters\n",
    "drseuss_text = 'data/combinedText.txt'\n",
    "seed_length = 50\n",
<<<<<<< HEAD
    "epochs = 500\n",
    "batch_size=128\n",
    "modelList = [('LSTM',256,'True'), ('Dense',256,'relu'), ('Dropout',.2,''), \n",
    "             ('LSTM',128,'True'), ('Dense',128,'relu'), ('Dropout',.2,''), \n",
    "             ('LSTM', 64,'False'), ('Dense',64,'relu'), \n",
    "             ('Flatten','',''),('Dense',vocab_size,'softmax')]\n",
=======
    "epochs = 100\n",
    "batch_size=128\n",
    "modelList = [('LSTM',100,'True'), ('Dropout',.2,''), ('LSTM', 100,'False'), \n",
    "             ('Dense',100,'relu'), ('Dense',vocab_size,'softmax')]\n",
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
    "#Create the model name\n",
    "modelName = f'm_{length}'\n",
    "for layer in modelList:\n",
    "    modelName+= f'_{layer[0]}_{layer[1]}_{layer[2]}'\n",
    "modelName += '.h5'\n",
    "modelName\n",
    "\n",
    "#create tokenizer file name .pkl\n",
    "tokenizerName = 'toke' + modelName.replace('m','',1).split('.h5')[0] + '.pkl'\n",
    "print(f'drseuss_text: \\'{drseuss_text}\\'\\nseed_length: {seed_length}\\nepochs: {epochs}\\nbatch_size: {batch_size}'\n",
    "     f'\\nmodelName: {modelName}\\ntokenizerName: {tokenizerName}\\nmodelList: {modelList}')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yertle', 'the', 'turtle', 'on', 'the', 'far', 'away', 'island', 'of', 'sala', 'ma', 'sond', 'yertle', 'the', 'turtle', 'was', 'king', 'of', 'the', 'pond.', 'a', 'nice', 'little', 'pond.', 'it', 'was', 'clean.', 'it', 'was', 'neat.', 'the', 'water', 'was', 'warm.', 'there', 'was', 'plenty', 'to', 'eat.', 'the', 'turtles', 'had', 'everything', 'turtles', 'might', 'need.', 'and', 'they', 'were', 'all', 'happy.', 'quite', 'happy', 'indeed.', 'they', 'were.', 'untill', 'yertle', 'the', 'king', 'of', 'them', 'all', 'decided', 'the', 'kingdom', 'he', 'ruled', 'was', 'too', 'small.', 'im', 'ruler', 'said', 'yertle', 'of', 'all', 'that', 'i', 'see.', 'but', 'i', 'dont', 'see', 'enough.', 'thats', 'the', 'trouble', 'with', 'me.', 'with', 'this', 'stone', 'for', 'a', 'throne', 'i', 'look', 'down', 'on', 'my', 'pond', 'but', 'i', 'cannot', 'look', 'down', 'on', 'the', 'places', 'beyond.', 'this', 'throne', 'that', 'i', 'sit', 'on', 'is', 'too', 'too', 'low', 'down.', 'it', 'ought', 'to', 'be', 'higher.', 'he', 'said', 'with', 'a', 'frown.', 'if', 'i', 'could', 'sit', 'high', 'how', 'much', 'greater', 'id', 'be.', 'what', 'a', 'king.', 'id', 'be', 'ruler', 'of', 'all', 'that', 'i', 'see.', 'so', 'yertle', 'the', 'turtle', 'king', 'lifted', 'his', 'hand', 'and', 'yertle', 'the', 'turtle', 'king', 'gave', 'a', 'command.', 'he', 'ordered', 'nine', 'turtles', 'to', 'swim', 'to', 'his', 'stone', 'and', 'using', 'these', 'turtles', 'he', 'built', 'a', 'new', 'throne.', 'he', 'made', 'each', 'turtle', 'stand', 'on', 'another', 'ones', 'back', 'and', 'he', 'piled', 'them']\n",
      "Total Tokens: 16226\n",
      "Unique Tokens: 2829\n"
     ]
    }
   ],
=======
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# load document\n",
    "in_filename = 'data/combinedText.txt'\n",
    "tokens = load_doc(drseuss_text)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 180,
=======
   "execution_count": 106,
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 16175\n",
      "sequences: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "#the plus one is because the last val in the list will be the expected prediction. \n",
    "#Its our Y-train\n",
    "length = seed_length + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    #line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(seq)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "print(f'sequences: {type(sequences[0])}')\n",
    "\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame(sequences)\n",
    "# X = df.iloc[:,:-1]\n",
    "# y = df.iloc[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 181,
=======
   "execution_count": 107,
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# integer encode sequences of words\n",
    "#sequences = [str(i) for i in sequences]\n",
    "# print(f'tokenizer: {tokenizer}')\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "# print(f'tokenizer: {tokenizer}')\n",
    "sequences = tokenizer.texts_to_sequences(sequences)\n",
    "# print(f'sequences: {sequences}')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 182,
=======
   "execution_count": 108,
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2830"
      ]
     },
<<<<<<< HEAD
     "execution_count": 182,
=======
     "execution_count": 108,
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 183,
=======
   "execution_count": 109,
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_length: 50\n",
<<<<<<< HEAD
      "shape of X: (16175, 50)\n",
      "shape of y: (16175, 2830)\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
=======
      "y: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(sequences)\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
<<<<<<< HEAD
    "print(f'seq_length: {seq_length}\\nshape of X: {X.shape}\\nshape of y: {y.shape}')\n",
    "print(y[0])"
=======
    "print(f'seq_length: {seq_length}\\ny: {y}')"
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 184,
=======
   "execution_count": 110,
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.add(Embedding(2830, 50, input_length=50))\n",
<<<<<<< HEAD
      "model.add(LSTM(256, return_sequences=True))\n",
      "model.add(Dense(256, activation=relu))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(LSTM(128, return_sequences=True))\n",
      "model.add(Dense(128, activation=relu))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(LSTM(64, return_sequences=True))\n",
      "model.add(Dense(64, activation=relu))\n",
      "model.add(Flatten())\n",
=======
      "model.add(LSTM(100, return_sequences=True))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(LSTM(100, return_sequences=True))\n",
      "model.add(Dense(100, activation=relu))\n",
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
      "model.add(Dense(2830, activation=softmax))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
<<<<<<< HEAD
      "embedding_14 (Embedding)     (None, 50, 50)            141500    \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 50, 256)           314368    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 50, 256)           65792     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 50, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 50, 128)           197120    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 50, 128)           16512     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 50, 64)            49408     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 50, 64)            4160      \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 2830)              9058830   \n",
      "=================================================================\n",
      "Total params: 9,847,690\n",
      "Trainable params: 9,847,690\n",
=======
      "embedding_7 (Embedding)      (None, 50, 50)            141500    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 50, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50, 100)           10100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 50, 2830)          285830    \n",
      "=================================================================\n",
      "Total params: 578,230\n",
      "Trainable params: 578,230\n",
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, seq_length, input_length=seq_length))\n",
    "print(f'model.add(Embedding({vocab_size}, {seq_length}, input_length={seq_length}))')\n",
    "for layer in modelList:\n",
    "    if layer[0] == 'LSTM':\n",
    "        #model.add(LSTM(100, return_sequences=True))\n",
    "        (_, neurons, rsequences) = layer\n",
    "        model.add(LSTM(neurons, return_sequences=rsequences))\n",
    "        print(f'model.add(LSTM({neurons}, return_sequences={return_sequences}))')\n",
    "        \n",
    "    if layer[0] == 'Dropout':\n",
    "        #model.add(Dropout(0.2))\n",
    "        (_, dropout_rate, _) = layer\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        print(f'model.add(Dropout({dropout_rate}))')\n",
    "        \n",
    "    if layer[0] == 'Dense':\n",
    "        #model.add(Dense(100, activation='relu'))\n",
    "        (_, neurons, afunction) = layer\n",
    "        model.add(Dense(neurons, activation=afunction))\n",
    "        print(f'model.add(Dense({neurons}, activation={afunction}))')\n",
    "        \n",
<<<<<<< HEAD
    "    if layer[0] == 'Flatten':\n",
    "        model.add(Flatten())\n",
    "        print(f'model.add(Flatten())')\n",
    "        \n",
=======
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
    "#model.add(LSTM(100, return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(LSTM(100))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 185,
=======
   "execution_count": 24,
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 1/500\n",
      "16175/16175 [==============================] - 150s 9ms/step - loss: 6.7318 - acc: 0.0376\n",
      "Epoch 2/500\n",
      "16175/16175 [==============================] - 147s 9ms/step - loss: 6.3521 - acc: 0.0469\n",
      "Epoch 3/500\n",
      "16175/16175 [==============================] - 169s 10ms/step - loss: 6.1428 - acc: 0.0496\n",
      "Epoch 4/500\n",
      "16175/16175 [==============================] - 145s 9ms/step - loss: 5.8945 - acc: 0.0490\n",
      "Epoch 5/500\n",
      "16175/16175 [==============================] - 101s 6ms/step - loss: 5.6066 - acc: 0.0524\n",
      "Epoch 6/500\n",
      "13440/16175 [=======================>......] - ETA: 26s - loss: 5.2356 - acc: 0.0522"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-080faff18321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
=======
      "Epoch 1/100\n",
      "10793/10793 [==============================] - 21s 2ms/step - loss: 6.4315 - acc: 0.0363\n",
      "Epoch 2/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 6.0330 - acc: 0.0406\n",
      "Epoch 3/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 6.0007 - acc: 0.0412\n",
      "Epoch 4/100\n",
      "10793/10793 [==============================] - 23s 2ms/step - loss: 5.8764 - acc: 0.0411\n",
      "Epoch 5/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 5.7949 - acc: 0.0437\n",
      "Epoch 6/100\n",
      "10793/10793 [==============================] - 21s 2ms/step - loss: 5.6893 - acc: 0.0500\n",
      "Epoch 7/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 5.5542 - acc: 0.0574\n",
      "Epoch 8/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 5.4106 - acc: 0.0677\n",
      "Epoch 9/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 5.2841 - acc: 0.0735\n",
      "Epoch 10/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 5.1745 - acc: 0.0782\n",
      "Epoch 11/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 5.0856 - acc: 0.0825\n",
      "Epoch 12/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 5.0043 - acc: 0.0852\n",
      "Epoch 13/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 4.9229 - acc: 0.0925\n",
      "Epoch 14/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 4.8441 - acc: 0.0979\n",
      "Epoch 15/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 4.7696 - acc: 0.1042\n",
      "Epoch 16/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 4.6946 - acc: 0.1085\n",
      "Epoch 17/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 4.6266 - acc: 0.1117\n",
      "Epoch 18/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 4.5628 - acc: 0.1127\n",
      "Epoch 19/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 4.5056 - acc: 0.1166\n",
      "Epoch 20/100\n",
      "10793/10793 [==============================] - 120s 11ms/step - loss: 4.4426 - acc: 0.1215\n",
      "Epoch 21/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 4.3786 - acc: 0.1252\n",
      "Epoch 22/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 4.3170 - acc: 0.1280\n",
      "Epoch 23/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 4.2510 - acc: 0.1359\n",
      "Epoch 24/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 4.1911 - acc: 0.1364\n",
      "Epoch 25/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 4.1371 - acc: 0.1439\n",
      "Epoch 26/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 4.0621 - acc: 0.1492\n",
      "Epoch 27/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 4.0060 - acc: 0.1521\n",
      "Epoch 28/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.9368 - acc: 0.1591\n",
      "Epoch 29/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.8871 - acc: 0.1660\n",
      "Epoch 30/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.8260 - acc: 0.1692\n",
      "Epoch 31/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.7672 - acc: 0.1720\n",
      "Epoch 32/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.7153 - acc: 0.1813\n",
      "Epoch 33/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.6573 - acc: 0.1862\n",
      "Epoch 34/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.6051 - acc: 0.1972\n",
      "Epoch 35/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.5567 - acc: 0.2048\n",
      "Epoch 36/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.5008 - acc: 0.2119\n",
      "Epoch 37/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.4625 - acc: 0.2142\n",
      "Epoch 38/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.3962 - acc: 0.2278\n",
      "Epoch 39/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.3451 - acc: 0.2352\n",
      "Epoch 40/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.2974 - acc: 0.2477\n",
      "Epoch 41/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.2475 - acc: 0.2507\n",
      "Epoch 42/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.1988 - acc: 0.2608\n",
      "Epoch 43/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.1534 - acc: 0.2639\n",
      "Epoch 44/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.0978 - acc: 0.2788\n",
      "Epoch 45/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 3.0423 - acc: 0.2839\n",
      "Epoch 46/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.9991 - acc: 0.2925\n",
      "Epoch 47/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.9482 - acc: 0.3022\n",
      "Epoch 48/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.9065 - acc: 0.3095\n",
      "Epoch 49/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.8676 - acc: 0.3172\n",
      "Epoch 50/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.8205 - acc: 0.3266\n",
      "Epoch 51/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.7746 - acc: 0.3312\n",
      "Epoch 52/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 2.7239 - acc: 0.3442\n",
      "Epoch 53/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.6828 - acc: 0.3549\n",
      "Epoch 54/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.6488 - acc: 0.3587\n",
      "Epoch 55/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 2.6047 - acc: 0.3691\n",
      "Epoch 56/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.5667 - acc: 0.3751\n",
      "Epoch 57/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.5248 - acc: 0.3796\n",
      "Epoch 58/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.4850 - acc: 0.3924\n",
      "Epoch 59/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.4405 - acc: 0.4013\n",
      "Epoch 60/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 2.4105 - acc: 0.4059\n",
      "Epoch 61/100\n",
      "10793/10793 [==============================] - 25s 2ms/step - loss: 2.3664 - acc: 0.4161\n",
      "Epoch 62/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 2.3339 - acc: 0.4231\n",
      "Epoch 63/100\n",
      "10793/10793 [==============================] - 21s 2ms/step - loss: 2.3034 - acc: 0.4327\n",
      "Epoch 64/100\n",
      "10793/10793 [==============================] - 22s 2ms/step - loss: 2.2633 - acc: 0.4384\n",
      "Epoch 65/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.2364 - acc: 0.4451\n",
      "Epoch 66/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.2046 - acc: 0.4482\n",
      "Epoch 67/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.1639 - acc: 0.4610\n",
      "Epoch 68/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.1340 - acc: 0.4677\n",
      "Epoch 69/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.1025 - acc: 0.4781\n",
      "Epoch 70/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.0745 - acc: 0.4792\n",
      "Epoch 71/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.0502 - acc: 0.4877\n",
      "Epoch 72/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 2.0129 - acc: 0.4970\n",
      "Epoch 73/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.9794 - acc: 0.5033\n",
      "Epoch 74/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.9501 - acc: 0.5115\n",
      "Epoch 75/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.9255 - acc: 0.5164\n",
      "Epoch 76/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.8943 - acc: 0.5227\n",
      "Epoch 77/100\n",
      "10793/10793 [==============================] - 22s 2ms/step - loss: 1.8702 - acc: 0.5282\n",
      "Epoch 78/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.8464 - acc: 0.5318\n",
      "Epoch 79/100\n",
      "10793/10793 [==============================] - 21s 2ms/step - loss: 1.8125 - acc: 0.5499\n",
      "Epoch 80/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.7937 - acc: 0.5498\n",
      "Epoch 81/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 1.7633 - acc: 0.5560\n",
      "Epoch 82/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 1.7285 - acc: 0.5648\n",
      "Epoch 83/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 1.7038 - acc: 0.5671\n",
      "Epoch 84/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 1.6848 - acc: 0.5695\n",
      "Epoch 85/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 1.6606 - acc: 0.5801\n",
      "Epoch 86/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.6271 - acc: 0.5881\n",
      "Epoch 87/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.6174 - acc: 0.5921\n",
      "Epoch 88/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.6044 - acc: 0.5895\n",
      "Epoch 89/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 1.5700 - acc: 0.6000\n",
      "Epoch 90/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.5434 - acc: 0.6059\n",
      "Epoch 91/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.5226 - acc: 0.6058\n",
      "Epoch 92/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.5050 - acc: 0.6158\n",
      "Epoch 93/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.4919 - acc: 0.6173\n",
      "Epoch 94/100\n",
      "10793/10793 [==============================] - 20s 2ms/step - loss: 1.4486 - acc: 0.6302\n",
      "Epoch 95/100\n",
      "10793/10793 [==============================] - 23s 2ms/step - loss: 1.4330 - acc: 0.6369\n",
      "Epoch 96/100\n",
      "10793/10793 [==============================] - 22s 2ms/step - loss: 1.4131 - acc: 0.6386\n",
      "Epoch 97/100\n",
      "10793/10793 [==============================] - 18s 2ms/step - loss: 1.3833 - acc: 0.6451\n",
      "Epoch 98/100\n",
      "10793/10793 [==============================] - 19s 2ms/step - loss: 1.3681 - acc: 0.6550\n",
      "Epoch 99/100\n",
      "10793/10793 [==============================] - 23s 2ms/step - loss: 1.3350 - acc: 0.6594\n",
      "Epoch 100/100\n",
      "10793/10793 [==============================] - 24s 2ms/step - loss: 1.3194 - acc: 0.6648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1824643908>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 157,
=======
   "execution_count": 25,
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "modelName = f'm_{length}'\n",
    "for layer in modelList:\n",
    "    modelName+= f'_{layer[0]}_{layer[1]}_{layer[2]}'\n",
    "modelName += '.h5'\n",
    "\n",
    "# save the model to file\n",
    "model.save(modelName)\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open(tokenizerName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 158,
=======
   "execution_count": 27,
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 159,
=======
   "execution_count": 28,
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "#def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 160,
=======
   "execution_count": 29,
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model(modelName)\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = load(open(tokenizerName, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 161,
=======
   "execution_count": 114,
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whosever room this is should be ashamed!\n",
      "His underwear is hanging on the lamp.\n",
      "His raincoat is there in the overstuffed chair,\n",
      "And the chair is becoming quite mucky and damp.\n",
      "His workbook is wedged in the window,\n",
      "His sweater's been thrown on the floor.\n",
      "His scarf and one ski are\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "# seed_text = lines[randint(0,len(lines))]\n",
    "seed_text = '''Whosever room this is should be ashamed!\n",
    "His underwear is hanging on the lamp.\n",
    "His raincoat is there in the overstuffed chair,\n",
    "And the chair is becoming quite mucky and damp.\n",
    "His workbook is wedged in the window,\n",
    "His sweater's been thrown on the floor.\n",
    "His scarf and one ski are'''\n",
    "seed_text = ' '.join(seed_text.split(' ')[0:50])\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 162,
=======
   "execution_count": 117,
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode our seed\n",
    "encoded = tokenizer.texts_to_sequences([seed_text])[0]\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 163,
=======
   "execution_count": 40,
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
=======
      "youre but whoville without clover by bipping their voices hour their snoots and the air murmured horton never thought all he laughed as their shoes but not started to haul him into a beezlenut almost an called an bags and floors saved all whoville ruckus and roar they do sniff\n"
>>>>>>> 1f86ecfdf9285e64ec796359d41e17f025eb8676
     ]
    }
   ],
   "source": [
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, seed_length)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
