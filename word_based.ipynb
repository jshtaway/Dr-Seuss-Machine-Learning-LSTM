{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, sys, os, pandas as pd\n",
    "from random import randint\n",
    "from pickle import dump, load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    tokens = text.split()\n",
    "    print(tokens[:100])\n",
    "    print('Total Tokens: %d' % len(tokens))\n",
    "    print('Unique Tokens: %d' % len(set(tokens)))\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize into sequences of tokens\n",
    "#the plus one is because the last val in the list will be the expected prediction. \n",
    "#Its our Y-train\n",
    "def sequencesCreate(length, tokens):\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    sequences = list()\n",
    "    for i in range(length, len(tokens)):\n",
    "        # select sequence of tokens\n",
    "        seq = tokens[i-length:i]\n",
    "        # convert into a line\n",
    "        #line = ' '.join(seq)\n",
    "        # store\n",
    "        sequences.append(seq)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    print(f'sequences[0][0]: {sequences[0][0]}')\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    # integer encode sequences of words\n",
    "    #sequences = [str(i) for i in sequences]\n",
    "    # print(f'tokenizer: {tokenizer}')\n",
    "    tokenizer.fit_on_texts(sequences)\n",
    "    # print(f'tokenizer: {tokenizer}')\n",
    "    sequences = tokenizer.texts_to_sequences(sequences)\n",
    "    # print(f'sequences: {sequences}')\n",
    "    \n",
    "    return sequences, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelFit(model, modelName, X, y, seq_length, batch_size, epochs, results_path):\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # define the checkpoint\n",
    "    filepath=f\"{results_path.rstrip('/')}/wi_{{epoch:02d}}_{{loss:.4f}}_{modelName}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    # fit model\n",
    "    history_callback = model.fit(X, y, batch_size=batch_size, epochs=epochs, callbacks=callbacks_list)\n",
    "    return history_callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- --- ---- --- ---- --- ---- ---- --- ----- ---- ---\n",
    "# -- Write Files ---- ---- ---- --- ---- --- --- --- -- \n",
    "#--- --- ---- --- ---- --- ---- ---- --- ----- ---- ---\n",
    "def writeFiles(model, modelName, history_callback, modelList, seq_length, total_sequences, epochs, batch_size, results_path):\n",
    "    loss_history = history_callback.history\n",
    "    \n",
    "    # save the model to file\n",
    "    model.save(results_path.rstrip('/') + '/m_' + modelName + '.h5')\n",
    "    loss_history['seq_length'] = seq_length\n",
    "    loss_history['total_sequences'] = total_sequences\n",
    "    loss_history['batch_size'] = batch_size\n",
    "    loss_history['epochs'] = epochs\n",
    "    \n",
    "    # save losses\n",
    "    with open(results_path.rstrip('/') + '/info_' + modelName + '.txt', 'w+') as f:\n",
    "        f.write(str(modelList))\n",
    "        f.write('\\n')\n",
    "        f.write(str(loss_history))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whosever room this is should be ashamed!\n",
      "His underwear is hanging on the lamp.\n",
      "His raincoat is there in the overstuffed chair,\n",
      "And the chair is becoming quite mucky and damp.\n",
      "His workbook is wedged in the window,\n",
      "His sweater's been thrown on the floor.\n",
      "His scarf and one ski are\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "# seed_text = lines[randint(0,len(lines))]\n",
    "seed_text = '''Whosever room this is should be ashamed!\n",
    "His underwear is hanging on the lamp.\n",
    "His raincoat is there in the overstuffed chair,\n",
    "And the chair is becoming quite mucky and damp.\n",
    "His workbook is wedged in the window,\n",
    "His sweater's been thrown on the floor.\n",
    "His scarf and one ski are'''\n",
    "\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "def defineModel(vocab_size, seq_length, modelList, length, input_shape):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import Dropout\n",
    "    from keras.layers import LSTM\n",
    "    from keras.utils import np_utils\n",
    "    from keras.layers import Embedding, Flatten\n",
    "    model = Sequential()\n",
    "    #-- EMBEDDED LAYER --- --- --- ---- --\n",
    "    #input_dim: size of the vocabulary in the text data.\n",
    "    #output_dim: size of the vector space where words will be embedded. or size of the output vectors from this layer try 32 or 100 or larger\n",
    "    #input_length: length of input seq's. ex: if input documents are comprised of 1000 words, it would be 1000.\n",
    "#     modelList = [{'model':'Embedding', 'input_dim':vocab_size, 'output_dim': 100, 'input_length': seq_length},\n",
    "#                  {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': .2}, \n",
    "#                  {'model': 'Dense','units':64,'activation':'relu'}, \n",
    "#                  {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': .2}, \n",
    "#                  {'model': 'Dense','units':64,'activation':'relu'}, \n",
    "#                  {'model':'Flatten'},\n",
    "#                  {'model': 'Dense','units':vocab_size,'activation':'softmax'},\n",
    "#                 ]\n",
    "    for i,layer in enumerate(modelList):\n",
    "        if layer['model'] == 'Embedding': \n",
    "            model.add(Embedding(input_dim=layer['input_dim'], output_dim=layer['output_dim'], \n",
    "                                input_length=layer['input_length']))\n",
    "\n",
    "            print(f\"model.add(Embedding(input_dim= {layer['input_dim']}, output_dim={ layer['output_dim'] }, input_length={ layer['input_length'] }))\")\n",
    "        elif layer['model'] == 'LSTM':\n",
    "            #model.add(LSTM(100, return_sequences=True))\n",
    "            #model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, input_dim=1))\n",
    "            model.add(LSTM(units=layer['units'], use_bias=layer['use_bias'], \n",
    "                           dropout=layer['dropout'], recurrent_dropout=layer['recurrent_dropout'], \n",
    "                           return_sequences = layer['return_sequences']))\n",
    "            print(f\"model.add(LSTM(units={layer['units']}, use_bias={layer['use_bias']}, dropout={layer['dropout']}, recurrent_dropout={layer['recurrent_dropout']} ))\")\n",
    "\n",
    "        elif layer['model'] == 'Dropout':\n",
    "            #model.add(Dropout(0.2))\n",
    "            model.add(Dropout(layer['dropout_rate']))\n",
    "            print(f\"model.add(Dropout({layer['dropout_rate']}))\")\n",
    "\n",
    "        elif layer['model'] == 'Dense':\n",
    "            #{'model': 'Dense','units':64,'activation':relu'}, \n",
    "            #model.add(Dense(100, activation='relu'))\n",
    "            model.add(Dense(units=layer['units'], activation=layer['activation']))\n",
    "            print(f\"model.add(Dense(units={layer['units']}, activation={layer['activation']}))\")\n",
    "\n",
    "        elif layer['model'] == 'Flatten':\n",
    "            model.add(Flatten())\n",
    "            print(f'model.add(Flatten())')\n",
    "        else:\n",
    "            raise IOError ('invalid layer')\n",
    "        \n",
    "    #Create the model name\n",
    "    import datetime\n",
    "    now = datetime.datetime.now()\n",
    "    modelName = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "    try:\n",
    "        print(model.summary())\n",
    "    except:\n",
    "        pass\n",
    "    return model, modelName\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModelComplete(results_path):\n",
    "    from keras.utils import to_categorical\n",
    "    \n",
    "    #--- PARAMETERS --- --- --- ---- --- --- ---- ---- --- ----- --- --- ----\n",
    "    #notes from website:\n",
    "    #-- Common values are 50, 100, and 300. We will use 50 here, --\n",
    "    #-- but consider testing smaller or larger values. --\n",
    "    #-- We will use a two LSTM hidden layers with 100 memory cells each. --\n",
    "    #-- More memory cells and a deeper network may achieve better results. --\n",
    "    drseuss_text = 'data/combinedText.txt'\n",
    "    seed_length = 50\n",
    "    length = seed_length + 1\n",
    "    epochs = 50\n",
    "    batch_size = 128\n",
    "    #-- ---- ---- --- ---- ----- ---- ----- ---- ----- ----- ---- ---- ---- ----\n",
    "    \n",
    "    #-- load document --- --- --- --- --\n",
    "    drseuss_text = 'data/combinedText.txt'\n",
    "    tokens = load_doc(drseuss_text)\n",
    "\n",
    "    #-- Create sequencer and tokenizer -- --- --- --- --- --- --- --- \n",
    "    sequences, tokenizer = sequencesCreate(length, tokens)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    #-- Creating X, y -- --- --- --- --- --- --- -- --\n",
    "    df = pd.DataFrame(sequences)\n",
    "    print(f'sequences:\\n{df.head(5)}')\n",
    "    X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "    seq_length = X.shape[1]\n",
    "    input_shape = X.shape\n",
    "    #-- One hot encoding -- --- --- --- --- --- -\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "    print(f'seq_length: {seq_length}\\nshape of X: {X.shape}\\nshape of y: {y.shape}')\n",
    "    #-- -- ---- --- --- --- --- --- ---- --- --- --- --\n",
    "\n",
    "    #-- Model List --- --- --- --- --- --- --- --- --- -- ---- --- --- --- ---- -- --\n",
    "#     modelList = [{'model':'Embedding', 'input_dim':vocab_size, 'output_dim': 256, 'input_length': seq_length},\n",
    "#                  {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': 0, 'return_sequences': True}, \n",
    "#                  {'model': 'Dense','units':64,'activation':'relu'}, \n",
    "#                  {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': 0, 'return_sequences': True}, \n",
    "#                  {'model': 'Dense','units':64,'activation':'relu'}, \n",
    "#                  {'model':'Flatten'},\n",
    "#                  {'model': 'Dense','units':vocab_size,'activation':'softmax'},\n",
    "#                 ]\n",
    "    modelList = [{'model':'Embedding', 'input_dim':vocab_size, 'output_dim': 256, 'input_length': seq_length},\n",
    "                 {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': 0, 'return_sequences': True}, \n",
    "                 {'model': 'Dense','units':100,'activation':'relu'}, \n",
    "                 {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': 0, 'return_sequences': True}, \n",
    "                 {'model': 'Dense','units':100,'activation':'relu'}, \n",
    "                 {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': 0, 'return_sequences': True}, \n",
    "                 {'model': 'Dense','units':100,'activation':'relu'}, \n",
    "                 {'model':'Flatten'},\n",
    "                 {'model': 'Dense','units':vocab_size,'activation':'softmax'},\n",
    "                ]\n",
    "\n",
    "    #-- --- ---- --- ---- --- --- ---- --- ---- --- ---- --- ---- --- --- --- --- ---\n",
    "    \n",
    "    print(f'drseuss_text: \\'{drseuss_text}\\'\\nseed_length: {seed_length}\\nepochs: {epochs}\\nbatch_size: {batch_size}'\n",
    "     f'\\nmodelList: {modelList}')\n",
    "    \n",
    "    #-- Create Model -- --- --- --- ---- --- -- ---- --- --- --- --- --- --- ---- --- ---\n",
    "    model, modelName = defineModel(vocab_size, seq_length, modelList, length, input_shape)\n",
    "    #-- save the tokenizer --- --- --- ---- --- --- ---- --\n",
    "    dump(tokenizer, open('token_'+modelName+'.pkl', 'wb'))\n",
    "    #-- Fit model -- ---- --- --- --- ---- --- --- ---- --- --- --- --- --- --- --- --- \n",
    "    history_callback = modelFit(model, modelName, X, y, seq_length, batch_size, epochs, results_path)\n",
    "    #-- Save history and final model --- -\n",
    "    writeFiles(model, modelName, history_callback, modelList, seq_length, len(sequences), epochs, batch_size, results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "#def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "def generate_seq(modelName, tokenizerName, seq_length, seed_text, n_words):\n",
    "    from keras.models import load_model\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    # load the model\n",
    "    model = load_model(modelName)\n",
    "\n",
    "    # load the tokenizer\n",
    "    tokenizer = load(open(tokenizerName, 'rb'))\n",
    "    \n",
    "    #Make 50 words long\n",
    "    seed_text = ' '.join(seed_text.split(' ')[0:seq_length])\n",
    "    \n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    \n",
    "    del model\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelList = [{'model':'Embedding', 'input_dim':2830, 'output_dim': 256, 'input_length': 50},\n",
    "#                  {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': 0, 'return_sequences': True}, \n",
    "#                  {'model': 'Dense','units':64,'activation':'relu'}, \n",
    "#                  {'model': 'LSTM', 'units':256, 'use_bias':True, 'dropout':.2, 'recurrent_dropout': 0, 'return_sequences': True}, \n",
    "#                  {'model': 'Dense','units':64,'activation':'relu'}, \n",
    "#                  {'model':'Flatten'},\n",
    "#                  {'model': 'Dense','units':2830,'activation':'softmax'},\n",
    "#                 ]\n",
    "# history_callback = {'history':{'loss': [6.8130, 6.3438, 6.0809, 5.6680, 5.0674, 4.1888, 3.2263, 2.4416, 1.8358, 1.3483, 0.9936, 0.7174, 0.5278, 0.3948, 0.2838, 0.2132, 0.1515, 0.1078, 0.0862, 0.0653, 0.0591, 0.0499, 0.0395, 0.0275, 0.0271, 0.0293, 0.0370, 0.0441, 0.0782, 0.1003, 0.0644, 0.0407, 0.0296, 0.0202, 0.0133, 0.0067, 0.0048, 0.0053, 0.0050, 0.0076, 0.0120, 0.0162, 0.0466, 0.1344, 0.1101, 0.0600, 0.0288, 0.0118, 0.0063], \n",
    "#                     'acc':  [0.0366, 0.0477, 0.0514, 0.0527, 0.0647, 0.1239, 0.1239, 0.4201, 0.5374, 0.6495, 0.7304, 0.7957, 0.8472, 0.8845, 0.9168, 0.9394, 0.9593, 0.9714, 0.9791, 0.9854, 0.9866, 0.9900, 0.9918, 0.9948, 0.9946, 0.9945, 0.9913, 0.9897, 0.9782, 0.9701, 0.9821, 0.9881, 0.9925, 0.9952, 0.9975, 0.9991, 0.9997, 0.9996, 0.9996, 0.9987, 0.9984, 0.9962, 0.9854, 0.9570, 0.9661, 0.9805, 0.9917, 0.9974, 0.9993]}}\n",
    "# writeFiles('NULL', '2018-10-22_11-31', history_callback, modelList, 50, total_sequences = 16175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    trainModelComplete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainModelComplete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_create(filepath = '.'):\n",
    "    import os, ast, json, re, seed\n",
    "    datetime = {}\n",
    "    for filename in os.listdir(filepath):\n",
    "        #wi_01_6.7077__2018-10-22_09-29.hdf5\n",
    "        m = re.search('wi_(..)_(......)__*(....-..-..)_(..-..).hdf5', filename)\n",
    "        if m:\n",
    "            epoch, loss, date, time = m.group(1), m.group(2), m.group(3), m.group(4)\n",
    "            if date+'_'+time not in datetime.keys():\n",
    "                #print(f\"{date+'_'+time} not in KEYS: \\n{datetime.keys()}\")\n",
    "                tokenizer = filepath+f'/token_{date}_{time}.pkl'\n",
    "                try:\n",
    "                    with open(filepath+'/info_' + date+'_'+time + '.txt') as f:\n",
    "                        text = f.read()\n",
    "                    modelList = text.split(']')[0] + ']'\n",
    "                    modelHistory = '{' + ']'.join(text.split(']')[1:]).split('{')[1]\n",
    "                    print(f\"NEW DATA: {date+'_'+time}\")\n",
    "                    modelHistory = ast.literal_eval(modelHistory)\n",
    "                    modelList = ast.literal_eval(modelList)\n",
    "                    epochs = modelHistory['epochs']\n",
    "                except:\n",
    "                    modelList = []\n",
    "                    modelHistory = {}\n",
    "                datetime[date+'_'+time] = {'model_list': modelList,\n",
    "                                           'model_history': modelHistory,\n",
    "                                           'sequence_list': ['no_model_data']*(epochs+1)}\n",
    "                print(datetime)\n",
    "            datetime[date+'_'+time]['sequence_list'][int(epoch)] = generate_seq(os.path.join(filepath,filename), tokenizer, 50, seed.seed_text, 50)\n",
    "            print('\\n',filename, \": \",datetime[date+'_'+time]['sequence_list'][int(epoch)])\n",
    "    #-- Write JSON file of all model training data --- \n",
    "    jsonFile = 'Alldata.json'; i = '0'\n",
    "    #-- Determine JSON file name -- \n",
    "    while os.path.isfile(jsonFile):\n",
    "        i = str(int(i)+1)\n",
    "        jsonFile = f\"Alldata{i}.json\"\n",
    "    #-- Write JSON file -- --- ----\n",
    "    with open(jsonFile, 'w+') as fp:\n",
    "        json.dump(datetime, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wi_76_0.0010__51_LSTM_256_True_Dense_256_relu_Dropout_0.2__LSTM_128_True_Dense_128_relu_Dropout_0.2__LSTM_64_False_Dense_64_relu_Flatten___Dense_2830_softmax.hdf\n",
    "def jsonify_the_old_style_file(filepath = '.'):\n",
    "    import seed, re, os\n",
    "    tokenizer = 'toke_51_LSTM_256_True_Dense_256_relu_Dropout_0.2__LSTM_128_True_Dense_128_relu_Dropout_0.2__LSTM_64_False_Dense_64_relu_Flatten___Dense_2830_softmax.pkl'\n",
    "    jsondict = {'sequences': ['no_data']*112, 'model':None, 'loss': ['no_data']*112}\n",
    "    for filename in os.listdir(filepath):\n",
    "        m = re.search('wi_(..)_(......)__(.*).hdf5', filename)\n",
    "        if m and re.search('51_LSTM_256_True_Dense_256_relu_Dropout_0.2__LSTM_128_True_Dense_128_relu_Dropout_0.2__LSTM_64_False_Dense_64_relu_Flatten___Dense_2830_softmax', filename):\n",
    "            epoch, loss, modellist = m.group(1), m.group(2), m.group(3)\n",
    "            jsondict['model'] = modellist\n",
    "            jsondict['loss'][int(epoch)] = float(loss)\n",
    "            jsondict['sequences'][int(epoch)] = generate_seq(os.path.join(filepath,filename), tokenizer, 50, seed.seed_text, 50)\n",
    "            print(jsondict['sequences'][int(epoch)])\n",
    "    jsonFile = 'Alldata.json'; i = '0'\n",
    "    #-- Determine JSON file name -- \n",
    "    while os.path.isfile(jsonFile):\n",
    "        i = str(int(i)+1)\n",
    "        jsonFile = f\"Alldata{i}.json\"\n",
    "    #-- Write JSON file -- --- ----\n",
    "    with open(jsonFile, 'w+') as fp:\n",
    "        json.dump(datetime, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
